name: Homework Auto-Grader

on:
  push:
    branches: [ main, master, submit ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:  # Allow manual triggering

env:
  PYTHON_VERSION: '3.9'

jobs:
  autograder:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout student code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-html pytest-json-report pytest-timeout
        
        # Install student requirements if they exist
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        
        # Install common data science libraries
        pip install pandas numpy matplotlib seaborn scipy scikit-learn
        
    - name: Verify required files exist
      run: |
        echo "Checking for required files..."
        required_files=("main.py")
        missing_files=""
        
        for file in "${required_files[@]}"; do
          if [[ ! -f "$file" ]]; then
            missing_files="$missing_files $file"
          fi
        done
        
        if [[ -n "$missing_files" ]]; then
          echo "‚ùå Missing required files:$missing_files"
          echo "MISSING_FILES=true" >> $GITHUB_ENV
          echo "missing-files=$missing_files" >> $GITHUB_OUTPUT
        else
          echo "‚úÖ All required files present"
          echo "MISSING_FILES=false" >> $GITHUB_ENV
        fi
        
    - name: Run syntax check
      if: env.MISSING_FILES == 'false'
      run: |
        echo "Running syntax check..."
        python -m py_compile main.py || {
          echo "‚ùå Syntax errors found in main.py"
          echo "SYNTAX_ERRORS=true" >> $GITHUB_ENV
          exit 1
        }
        echo "‚úÖ No syntax errors found"
        echo "SYNTAX_ERRORS=false" >> $GITHUB_ENV
        
    - name: Run security scan
      if: env.MISSING_FILES == 'false' && env.SYNTAX_ERRORS == 'false'
      run: |
        echo "Running basic security scan..."
        # Check for dangerous imports/functions
        if grep -r "exec\|eval\|__import__\|subprocess\|os\.system" *.py 2>/dev/null; then
          echo "‚ö†Ô∏è  Warning: Potentially unsafe code detected"
          echo "SECURITY_WARNING=true" >> $GITHUB_ENV
        else
          echo "‚úÖ No obvious security issues found"
          echo "SECURITY_WARNING=false" >> $GITHUB_ENV
        fi
        
    - name: Run tests with grading
      if: env.MISSING_FILES == 'false' && env.SYNTAX_ERRORS == 'false'
      run: |
        echo "Running automated tests..."
        pytest tests/ -v \
          --html=test_report.html --self-contained-html \
          --json-report --json-report-file=test_results.json \
          --tb=short \
          --timeout=30 \
          --strict-markers \
          -x  # Stop on first failure for faster feedback
          
    - name: Generate grade report
      if: always() && env.MISSING_FILES == 'false'
      run: |
        python -c "
        import json
        import os
        
        def generate_grade_report():
            # Default scores
            total_score = 0
            max_score = 100
            details = []
            
            try:
                # Read test results if available
                if os.path.exists('test_results.json'):
                    with open('test_results.json', 'r') as f:
                        results = json.load(f)
                    
                    passed = results['summary']['passed']
                    failed = results['summary']['failed']
                    total_tests = passed + failed
                    
                    if total_tests > 0:
                        total_score = int((passed / total_tests) * 100)
                    
                    details.append(f'Tests passed: {passed}/{total_tests}')
                    details.append(f'Test success rate: {total_score}%')
                else:
                    details.append('No test results found')
            
            except Exception as e:
                details.append(f'Error reading test results: {e}')
            
            # Apply penalties
            penalties = []
            if os.getenv('MISSING_FILES') == 'true':
                penalties.append('Missing required files: -50 points')
                total_score = max(0, total_score - 50)
            
            if os.getenv('SYNTAX_ERRORS') == 'true':
                penalties.append('Syntax errors: -25 points')
                total_score = max(0, total_score - 25)
            
            if os.getenv('SECURITY_WARNING') == 'true':
                penalties.append('Security warning: -10 points')
                total_score = max(0, total_score - 10)
            
            # Generate report
            report = []
            report.append('# Homework Grade Report')
            report.append('')
            report.append(f'## Final Score: {total_score}/{max_score}')
            
            if total_score >= 90:
                report.append('üéâ Excellent work!')
            elif total_score >= 80:
                report.append('üëç Good job!')
            elif total_score >= 70:
                report.append('‚úÖ Meets requirements')
            elif total_score >= 60:
                report.append('‚ö†Ô∏è  Needs improvement')
            else:
                report.append('‚ùå Please review and resubmit')
            
            report.append('')
            report.append('## Details')
            for detail in details:
                report.append(f'- {detail}')
            
            if penalties:
                report.append('')
                report.append('## Penalties Applied')
                for penalty in penalties:
                    report.append(f'- {penalty}')
            
            return '\\n'.join(report), total_score
        
        report_content, final_score = generate_grade_report()
        
        with open('GRADE_REPORT.md', 'w') as f:
            f.write(report_content)
        
        print('GRADE REPORT')
        print('=' * 50)
        print(report_content)
        
        # Set outputs for other steps
        with open(os.environ['GITHUB_ENV'], 'a') as f:
            f.write(f'FINAL_SCORE={final_score}\\n')
        "
        
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          test_report.html
          test_results.json
          GRADE_REPORT.md
        retention-days: 30
        
    - name: Comment on PR with grade
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          try {
            const gradeReport = fs.readFileSync('GRADE_REPORT.md', 'utf8');
            const finalScore = process.env.FINAL_SCORE || '0';
            
            const comment = `## ü§ñ Automated Grading Results
            
            ${gradeReport}
            
            ---
            
            ### Next Steps
            ${finalScore >= 70 
              ? '‚úÖ Your assignment meets the requirements! Well done!' 
              : 'üìù Please review the feedback above and make improvements before the deadline.'}
            
            üí° **Tip**: Check the test results artifact for detailed feedback.
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not post comment:', error);
          }
          
    - name: Set status check
      if: always()
      uses: actions/github-script@v6
      with:
        script: |
          const finalScore = parseInt(process.env.FINAL_SCORE || '0');
          const state = finalScore >= 70 ? 'success' : 'failure';
          const description = `Score: ${finalScore}/100`;
          
          github.rest.repos.createCommitStatus({
            owner: context.repo.owner,
            repo: context.repo.repo,
            sha: context.sha,
            state: state,
            target_url: `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
            description: description,
            context: 'Homework Auto-Grader'
          });
          
    - name: Fail job if score too low
      if: always()
      run: |
        final_score=${FINAL_SCORE:-0}
        if [ "$final_score" -lt 70 ]; then
          echo "‚ùå Assignment score ($final_score/100) is below the passing threshold (70/100)"
          exit 1
        else
          echo "‚úÖ Assignment passed with score: $final_score/100"
        fi