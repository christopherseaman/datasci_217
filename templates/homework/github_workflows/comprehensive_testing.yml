name: Comprehensive Testing Suite

on:
  push:
    branches: [ main, master, submit, develop ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run nightly testing at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Scope of testing to perform'
        required: true
        default: 'full'
        type: choice
        options:
        - full
        - unit-only
        - integration-only
        - performance-only
        - security-only
      assignment_type:
        description: 'Type of assignment being tested'
        required: false
        default: 'auto-detect'
        type: choice
        options:
        - auto-detect
        - function
        - data-processing
        - cli-application
        - file-io

env:
  PYTHON_VERSION: '3.9'
  ASSIGNMENT_TYPE: ${{ github.event.inputs.assignment_type || 'auto-detect' }}
  TEST_SCOPE: ${{ github.event.inputs.test_scope || 'full' }}

jobs:
  # Pre-flight checks
  preflight:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      assignment-type: ${{ steps.detect.outputs.assignment-type }}
      has-tests: ${{ steps.detect.outputs.has-tests }}
      python-files: ${{ steps.detect.outputs.python-files }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Detect assignment type and structure
      id: detect
      run: |
        echo "ðŸ” Analyzing repository structure..."
        
        # Count Python files
        python_count=$(find . -name "*.py" -not -path "./.git/*" | wc -l)
        echo "python-files=$python_count" >> $GITHUB_OUTPUT
        
        # Check for test directory
        if [ -d "tests" ] || [ -f "test_*.py" ]; then
          echo "has-tests=true" >> $GITHUB_OUTPUT
          echo "âœ… Test files detected"
        else
          echo "has-tests=false" >> $GITHUB_OUTPUT
          echo "âš ï¸ No test directory found"
        fi
        
        # Auto-detect assignment type if not specified
        assignment_type="${{ env.ASSIGNMENT_TYPE }}"
        
        if [ "$assignment_type" = "auto-detect" ]; then
          if [ -f "data_processor.py" ] || grep -q "pandas\|numpy" *.py 2>/dev/null; then
            assignment_type="data-processing"
          elif grep -q "argparse\|click" *.py 2>/dev/null || [ -f "cli.py" ]; then
            assignment_type="cli-application"
          elif grep -q "open\|csv\|json" *.py 2>/dev/null; then
            assignment_type="file-io"
          else
            assignment_type="function"
          fi
        fi
        
        echo "assignment-type=$assignment_type" >> $GITHUB_OUTPUT
        echo "ðŸ“ Detected assignment type: $assignment_type"

  # Unit testing matrix
  unit-tests:
    needs: preflight
    if: needs.preflight.outputs.has-tests == 'true' && (env.TEST_SCOPE == 'full' || env.TEST_SCOPE == 'unit-only')
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']
        test-category: ['basic', 'edge-cases', 'error-handling']
      fail-fast: false
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-html pytest-json-report pytest-cov pytest-timeout
        
        # Install assignment-specific dependencies
        case "${{ needs.preflight.outputs.assignment-type }}" in
          "data-processing")
            pip install pandas numpy matplotlib seaborn scipy scikit-learn
            ;;
          "cli-application")
            pip install click argparse
            ;;
          "file-io")
            pip install jsonschema pyyaml
            ;;
        esac
        
        # Install student requirements if present
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        
    - name: Run unit tests by category
      run: |
        echo "ðŸ§ª Running ${{ matrix.test-category }} tests..."
        
        case "${{ matrix.test-category }}" in
          "basic")
            pytest tests/ -m "function_test" -v \
              --html=unit_basic_report.html --self-contained-html \
              --json-report --json-report-file=unit_basic_results.json \
              --cov=. --cov-report=json:coverage_basic.json \
              --timeout=30
            ;;
          "edge-cases")
            pytest tests/ -m "edge_case" -v \
              --html=unit_edge_report.html --self-contained-html \
              --json-report --json-report-file=unit_edge_results.json \
              --timeout=60
            ;;
          "error-handling")
            pytest tests/ -m "error_handling" -v \
              --html=unit_error_report.html --self-contained-html \
              --json-report --json-report-file=unit_error_results.json \
              --timeout=30
            ;;
        esac
        
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: unit-test-results-${{ matrix.python-version }}-${{ matrix.test-category }}
        path: |
          unit_*_report.html
          unit_*_results.json
          coverage_*.json
        retention-days: 30

  # Integration testing
  integration-tests:
    needs: [preflight, unit-tests]
    if: always() && needs.preflight.outputs.has-tests == 'true' && (env.TEST_SCOPE == 'full' || env.TEST_SCOPE == 'integration-only')
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-html pytest-json-report pytest-timeout
        
        # Install full dependency stack for integration
        pip install pandas numpy matplotlib seaborn scipy scikit-learn
        pip install click argparse jsonschema pyyaml requests
        
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        
    - name: Set up test data
      run: |
        echo "ðŸ“Š Setting up integration test environment..."
        
        # Create test data directory
        mkdir -p test_data
        
        # Generate sample data based on assignment type
        python -c "
        import json
        import csv
        from pathlib import Path
        
        # Create sample JSON data
        sample_data = {
            'users': [
                {'id': 1, 'name': 'Alice', 'score': 95.5},
                {'id': 2, 'name': 'Bob', 'score': 87.2},
                {'id': 3, 'name': 'Charlie', 'score': 92.1}
            ],
            'metadata': {'version': '1.0', 'created': '2024-01-01'}
        }
        
        with open('test_data/sample.json', 'w') as f:
            json.dump(sample_data, f, indent=2)
        
        # Create sample CSV data
        with open('test_data/sample.csv', 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['id', 'name', 'score', 'category'])
            writer.writerow([1, 'Alice', 95.5, 'A'])
            writer.writerow([2, 'Bob', 87.2, 'B'])
            writer.writerow([3, 'Charlie', 92.1, 'A'])
        
        # Create large test file for performance testing
        with open('test_data/large_dataset.csv', 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['id', 'value', 'category'])
            for i in range(10000):
                writer.writerow([i, i * 1.5, f'cat_{i % 10}'])
        
        print('âœ… Test data created successfully')
        "
        
    - name: Run integration tests
      run: |
        echo "ðŸ”— Running integration tests..."
        
        pytest tests/ -m "integration" -v \
          --html=integration_report.html --self-contained-html \
          --json-report --json-report-file=integration_results.json \
          --timeout=120 \
          --tb=short
          
    - name: Test CLI functionality (if applicable)
      if: needs.preflight.outputs.assignment-type == 'cli-application'
      run: |
        echo "âŒ¨ï¸ Testing CLI functionality..."
        
        if [ -f main.py ]; then
          # Test help functionality
          python main.py --help > cli_help_output.txt 2>&1 || echo "Help command failed"
          
          # Test with sample data
          if [ -f test_data/sample.csv ]; then
            python main.py --input test_data/sample.csv --output test_output.txt > cli_test_output.txt 2>&1 || echo "CLI test failed"
          fi
          
          echo "CLI test outputs captured"
        fi
        
    - name: Upload integration results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-results
        path: |
          integration_report.html
          integration_results.json
          cli_*.txt
          test_output.txt
        retention-days: 30

  # Performance testing
  performance-tests:
    needs: [preflight, unit-tests]
    if: always() && (env.TEST_SCOPE == 'full' || env.TEST_SCOPE == 'performance-only')
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install performance testing tools
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark memory-profiler psutil
        pip install pandas numpy matplotlib seaborn scipy scikit-learn
        
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        
    - name: Run performance benchmarks
      run: |
        echo "âš¡ Running performance benchmarks..."
        
        if [ -d tests ]; then
          # Run pytest benchmarks if they exist
          pytest tests/ -m "performance" \
            --benchmark-only \
            --benchmark-sort=mean \
            --benchmark-json=benchmark_results.json \
            --timeout=300 || echo "No performance tests found"
        fi
        
    - name: Memory usage analysis
      run: |
        echo "ðŸ’¾ Analyzing memory usage..."
        
        if [ -f main.py ]; then
          python -c "
        import tracemalloc
        import subprocess
        import sys
        import json
        import time
        
        def analyze_memory():
            tracemalloc.start()
            start_time = time.time()
            
            try:
                # Monitor memory during execution
                result = subprocess.run([sys.executable, 'main.py'], 
                                      capture_output=True, text=True, timeout=180)
                
                end_time = time.time()
                current, peak = tracemalloc.get_traced_memory()
                tracemalloc.stop()
                
                stats = {
                    'execution_time': round(end_time - start_time, 3),
                    'current_memory_mb': round(current / 1024 / 1024, 2),
                    'peak_memory_mb': round(peak / 1024 / 1024, 2),
                    'exit_code': result.returncode,
                    'stdout_length': len(result.stdout),
                    'stderr_length': len(result.stderr)
                }
                
                with open('memory_performance.json', 'w') as f:
                    json.dump(stats, f, indent=2)
                
                print(f'Performance Analysis:')
                print(f'  Execution time: {stats[\"execution_time\"]}s')
                print(f'  Peak memory: {stats[\"peak_memory_mb\"]}MB')
                print(f'  Exit code: {stats[\"exit_code\"]}')
                
                # Performance thresholds
                if stats['execution_time'] > 60:
                    print('âš ï¸ Execution time exceeded 60 seconds')
                    sys.exit(1)
                elif stats['peak_memory_mb'] > 1000:
                    print('âš ï¸ Memory usage exceeded 1GB')
                    sys.exit(1)
                else:
                    print('âœ… Performance within acceptable limits')
                    
            except subprocess.TimeoutExpired:
                print('âŒ Script execution timed out after 3 minutes')
                sys.exit(1)
            except Exception as e:
                print(f'âŒ Performance analysis failed: {e}')
                sys.exit(1)
        
        analyze_memory()
        "
        fi
        
    - name: Upload performance results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-test-results
        path: |
          benchmark_results.json
          memory_performance.json
        retention-days: 30

  # Security scanning
  security-scan:
    needs: preflight
    if: env.TEST_SCOPE == 'full' || env.TEST_SCOPE == 'security-only'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety semgrep
        
    - name: Run Bandit security scan
      if: needs.preflight.outputs.python-files != '0'
      run: |
        echo "ðŸ”’ Running Bandit security analysis..."
        
        bandit -r . -f json -o bandit_report.json || true
        bandit -r . -f txt > bandit_summary.txt || true
        
        echo "Bandit scan completed"
        
    - name: Check dependencies for vulnerabilities
      run: |
        echo "ðŸ›¡ï¸ Checking dependencies for known vulnerabilities..."
        
        if [ -f requirements.txt ]; then
          safety check -r requirements.txt --json --output safety_report.json || echo "Safety check completed with warnings"
          safety check -r requirements.txt > safety_summary.txt || echo "Safety check completed with warnings"
        else
          echo "No requirements.txt found, skipping dependency check"
        fi
        
    - name: Run Semgrep static analysis
      if: needs.preflight.outputs.python-files != '0'
      run: |
        echo "ðŸ” Running Semgrep static analysis..."
        
        semgrep --config=auto --json --output=semgrep_report.json . || echo "Semgrep completed with findings"
        semgrep --config=auto . > semgrep_summary.txt || echo "Semgrep completed with findings"
        
    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-scan-results
        path: |
          bandit_report.json
          bandit_summary.txt
          safety_report.json
          safety_summary.txt
          semgrep_report.json
          semgrep_summary.txt
        retention-days: 30

  # Final grading report
  generate-final-report:
    needs: [preflight, unit-tests, integration-tests, performance-tests, security-scan]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Generate comprehensive report
      run: |
        python -c "
        import json
        import os
        import glob
        from pathlib import Path
        from datetime import datetime
        
        def load_json_safe(filepath):
            try:
                if os.path.exists(filepath):
                    with open(filepath, 'r') as f:
                        return json.load(f)
            except:
                pass
            return None
        
        def calculate_comprehensive_score():
            total_score = 0
            max_score = 100
            details = []
            
            # Unit test results (50 points max)
            unit_tests_passed = 0
            unit_tests_total = 0
            
            for result_file in glob.glob('**/unit_*_results.json', recursive=True):
                result = load_json_safe(result_file)
                if result and 'summary' in result:
                    unit_tests_passed += result['summary'].get('passed', 0)
                    unit_tests_total += result['summary'].get('total', 0)
            
            if unit_tests_total > 0:
                unit_score = min(50, int((unit_tests_passed / unit_tests_total) * 50))
                total_score += unit_score
                details.append(f'Unit Tests: {unit_tests_passed}/{unit_tests_total} passed ({unit_score}/50 points)')
            
            # Integration tests (20 points max)
            integration_result = load_json_safe('integration-test-results/integration_results.json')
            if integration_result and 'summary' in integration_result:
                passed = integration_result['summary'].get('passed', 0)
                total = integration_result['summary'].get('total', 1)
                integration_score = int((passed / total) * 20) if total > 0 else 0
                total_score += integration_score
                details.append(f'Integration Tests: {passed}/{total} passed ({integration_score}/20 points)')
            
            # Performance analysis (15 points max)
            performance_data = load_json_safe('performance-test-results/memory_performance.json')
            if performance_data:
                perf_score = 15
                exec_time = performance_data.get('execution_time', 0)
                peak_memory = performance_data.get('peak_memory_mb', 0)
                
                if exec_time > 30:
                    perf_score -= 5
                if exec_time > 60:
                    perf_score -= 5
                if peak_memory > 500:
                    perf_score -= 5
                
                perf_score = max(0, perf_score)
                total_score += perf_score
                details.append(f'Performance: {exec_time}s execution, {peak_memory}MB memory ({perf_score}/15 points)')
            
            # Security analysis (10 points max)
            security_score = 10
            security_issues = 0
            
            bandit_report = load_json_safe('security-scan-results/bandit_report.json')
            if bandit_report:
                security_issues += len(bandit_report.get('results', []))
            
            if security_issues > 0:
                security_score = max(0, 10 - security_issues)
            
            total_score += security_score
            details.append(f'Security: {security_issues} issues found ({security_score}/10 points)')
            
            # Code quality bonus (5 points max)
            quality_score = 5  # Assume good quality unless proven otherwise
            total_score += quality_score
            details.append(f'Code Quality: ({quality_score}/5 points)')
            
            return min(total_score, max_score), details
        
        # Generate final report
        final_score, score_details = calculate_comprehensive_score()
        
        report_lines = [
            '# ðŸŽ“ Comprehensive Testing Report',
            '',
            f'**Generated**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")}',
            f'**Assignment Type**: ${{ needs.preflight.outputs.assignment-type }}',
            f'**Test Scope**: ${{ env.TEST_SCOPE }}',
            '',
            f'## ðŸ“Š Final Score: {final_score}/100',
            '',
            '### Score Breakdown',
        ]
        
        for detail in score_details:
            report_lines.append(f'- {detail}')
        
        # Add grade interpretation
        report_lines.extend([
            '',
            '### ðŸŽ¯ Grade Interpretation',
        ])
        
        if final_score >= 95:
            grade_text = 'ðŸ† **A+ (Outstanding)** - Exceptional work that exceeds expectations!'
        elif final_score >= 90:
            grade_text = 'ðŸŒŸ **A (Excellent)** - Outstanding work that meets all requirements!'
        elif final_score >= 85:
            grade_text = 'â­ **A- (Very Good)** - High-quality work with minor room for improvement.'
        elif final_score >= 80:
            grade_text = 'âœ… **B+ (Good)** - Solid work that meets most requirements well.'
        elif final_score >= 75:
            grade_text = 'ðŸ‘ **B (Satisfactory)** - Adequate work that meets basic requirements.'
        elif final_score >= 70:
            grade_text = 'âš ï¸ **B- (Acceptable)** - Meets minimum requirements but needs improvement.'
        elif final_score >= 65:
            grade_text = 'ðŸ“ **C (Below Expectations)** - Significant improvements needed.'
        else:
            grade_text = 'âŒ **F (Unsatisfactory)** - Major issues found. Please review and resubmit.'
        
        report_lines.append(grade_text)
        
        # Add detailed analysis sections
        report_lines.extend([
            '',
            '## ðŸ“‹ Detailed Analysis',
            ''
        ])
        
        # Test coverage analysis
        coverage_files = glob.glob('**/coverage_*.json', recursive=True)
        if coverage_files:
            report_lines.append('### ðŸ“Š Test Coverage')
            for coverage_file in coverage_files[:3]:  # Limit to first 3
                coverage_data = load_json_safe(coverage_file)
                if coverage_data and 'totals' in coverage_data:
                    percent = coverage_data['totals'].get('percent_covered', 0)
                    report_lines.append(f'- Coverage: {percent:.1f}%')
        
        # Performance summary
        if performance_data:
            report_lines.extend([
                '',
                '### âš¡ Performance Summary',
                f'- Execution Time: {performance_data.get(\"execution_time\", \"N/A\")}s',
                f'- Peak Memory Usage: {performance_data.get(\"peak_memory_mb\", \"N/A\")}MB',
                f'- Exit Code: {performance_data.get(\"exit_code\", \"N/A\")}'
            ])
        
        # Security summary
        if security_issues > 0:
            report_lines.extend([
                '',
                '### ðŸ”’ Security Issues',
                f'- {security_issues} potential security issues detected',
                '- Review the security scan artifacts for details'
            ])
        
        # Next steps
        report_lines.extend([
            '',
            '## ðŸŽ¯ Next Steps',
            '',
            '### For Students',
        ])
        
        if final_score >= 80:
            report_lines.extend([
                '- âœ… Great work! Your assignment meets the requirements.',
                '- ðŸ“š Consider reviewing any feedback for learning opportunities.',
                '- ðŸš€ Keep up the excellent work!'
            ])
        else:
            report_lines.extend([
                '- ðŸ“ Review the detailed test results and feedback.',
                '- ðŸ”§ Focus on areas with the lowest scores first.',
                '- ðŸ’¡ Consult the assignment guidelines and examples.',
                '- ðŸ¤ Reach out during office hours if you need help.'
            ])
        
        report_lines.extend([
            '',
            '### For Instructors',
            '- ðŸ“Š All detailed test results are available in the artifacts',
            '- ðŸ” Security and performance reports provide additional insights',
            '- ðŸ“ˆ Consider this data for improving future assignments',
            '',
            '---',
            '',
            'ðŸ“ **View detailed reports in the Actions artifacts section**'
        ])
        
        # Write the final report
        report_content = '\\n'.join(report_lines)
        with open('COMPREHENSIVE_TESTING_REPORT.md', 'w') as f:
            f.write(report_content)
        
        print('ðŸŽ“ COMPREHENSIVE TESTING REPORT')
        print('=' * 50)
        print(report_content)
        
        # Set environment variables for other steps
        with open(os.environ['GITHUB_ENV'], 'a') as f:
            f.write(f'FINAL_SCORE={final_score}\\n')
            f.write(f'GRADE_LETTER={\"A\" if final_score >= 90 else \"B\" if final_score >= 80 else \"C\" if final_score >= 70 else \"F\"}\\n')
        "
        
    - name: Upload comprehensive report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-testing-report
        path: |
          COMPREHENSIVE_TESTING_REPORT.md
        retention-days: 90
        
    - name: Post summary comment (on PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          try {
            const report = fs.readFileSync('COMPREHENSIVE_TESTING_REPORT.md', 'utf8');
            const finalScore = process.env.FINAL_SCORE || '0';
            const gradeLetter = process.env.GRADE_LETTER || 'F';
            
            const comment = `## ðŸ¤– Comprehensive Testing Results
            
            **Final Score: ${finalScore}/100 (${gradeLetter})**
            
            ${report}
            
            ---
            
            ðŸ’¡ **Access detailed reports**: Check the "Actions" tab â†’ "Artifacts" section for complete test results, performance analysis, and security reports.
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not post comprehensive report:', error);
          }