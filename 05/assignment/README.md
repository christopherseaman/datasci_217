# Assignment 5: Midterm Exam - Clinical Trial Data Analysis

Total: 100 points

## Scenario

You're analyzing data from a multi-site cardiovascular health clinical trial. The trial enrolled 10,000 patients across 5 hospital sites over 2 years, testing two different treatment interventions against a control group.

Your task: Build a complete data processing pipeline to clean, validate, and analyze the clinical trial data.

Dataset: `data/clinical_trial_raw.csv` (10,000 patients, 18 variables) - *Generated by Q1 setup script*

Configuration: `q2_config.txt` (Q2 data generation parameters)

## File Organization

Scaffolds have been provided, you will work with these files:

Scripts (complete the code):

1. `q1_setup_project.sh` - Shell script to create directories and generate dataset (Q1)
2. `q2_process_metadata.py` - python3 fundamentals and config processing (Q2)
3. `q3_data_utils.py` - **Core data utilities library** - 8 reusable functions (Q3)
4. `q8_run_pipeline.sh` - Pipeline automation script (Q8)

Notebooks (complete the analysis):

1. `q4_exploration.ipynb` - Data exploration using Q3 utilities (Q4)
2. `q5_missing_data.ipynb` - Missing data analysis (Q5)
3. `q6_transformation.ipynb` - Data transformation and feature engineering (Q6)
4. `q7_aggregation.ipynb` - Grouped analysis and reporting (Q7)

Key Design:

- **Q3 is your utility library** - Write 8 reusable pandas functions here
- **Q4-Q7 import from Q3** - Use your utilities in notebooks for real analysis
- **Q8 runs the pipeline** - Executes Q4-Q7 notebooks (Q3 is imported, not run directly)
- This mirrors professional data science workflows: utilities → analysis → automation

### Output structure:

```bash
output/           # Data artifacts (CSV, TXT files with results)
reports/          # Logs and metadata (pipeline execution info)
data/             # Input data (provided)
q2_config.txt     # Configuration file (Q2 data generation parameters)
```


## Variables:

### Demographics:

- `patient_id`: Unique patient identifier (P00001, P00002, ...)
- `age`: Patient age in years (0-100, filter to 18-85 for analysis)
- `sex`: Patient sex (M/F)
- `bmi`: Body Mass Index (kg/m²)
- `enrollment_date`: Date patient enrolled in trial (YYYY-MM-DD)

### Clinical Measurements:

- `systolic_bp`: Systolic blood pressure (mmHg)
- `diastolic_bp`: Diastolic blood pressure (mmHg)
- `cholesterol_total`: Total cholesterol (mg/dL)
- `cholesterol_hdl`: HDL ("good") cholesterol (mg/dL)
- `cholesterol_ldl`: LDL ("bad") cholesterol (mg/dL)
- `glucose_fasting`: Fasting blood glucose (mg/dL)

### Trial Information:

- `site`: Hospital site (Site A, Site B, Site C, Site D, Site E)
- `intervention_group`: Treatment group (control, Treatment A, Treatment B)
- `follow_up_months`: Months of follow-up (0-24)
- `adverse_events`: Number of adverse events (0+)

### Outcomes:

- `outcome_cvd`: Cardiovascular disease outcome (Yes/No)
- `adherence_pct`: Medication adherence percentage (0-100)
- `dropout`: Whether patient dropped out (Yes/No)

### Data Quality Issues

The generated dataset contains realistic clinical trial data quality issues that you'll need to handle:

- **Missing data**: 5-15% missing values (varies by site quality)
- **Sentinel values**: -999 for age, -1 for BMI (data entry system codes)
- **Text inconsistencies**: Mixed capitalization, typos, spacing issues
- **Date formats**: Multiple formats (YYYY-MM-DD, MM/DD/YYYY, DD-MM-YYYY)
- **Whitespace**: Leading/trailing spaces in text fields

## Getting Started

Complete the questions in order:

1. **Q1**: Set up project directories and generate the dataset
2. **Q2**: Complete Python fundamentals (standalone exercise)
3. **Q3**: Build data utilities library
4. **Q4-Q7**: Complete analysis notebooks (using the dataset from Q1)
5. **Q8**: Run the complete pipeline

## Question 1: Project Setup Script (10 points)

File: `q1_setup_project.sh`

Create an executable shell script that sets up the project structure.

### Requirements

Your script must:

1. Be executable (`chmod +x q1_setup_project.sh`)
2. Have shebang `#!/bin/bash`
3. Create `data/` directory
4. Create `output/` directory
5. Create `reports/` directory
6. Generate the dataset by running `python3 generate_data.py`
7. Save directory listing to `reports/directory_structure.txt` using `tree`

### Q1 Required Output

- Executable `q1_setup_project.sh`
- `reports/directory_structure.txt` - Directory listing showing created structure
- `data/clinical_trial_raw.csv` - Generated dataset (10,000 patients, 18 variables)
- Directories created

## Question 2: `python` Data Processing (25 points)

File: `q2_process_metadata.py`

Create an executable python3 script that processes a configuration file (`q2_config.txt`) for data generation. Note that this script focuses on `python` methods WITHOUT using `pandas`. 

**Important:** While Q2 is a standalone Python fundamentals exercise (not part of the main data pipeline Q3→Q7), it still produces required outputs that are used in grading.

The config file format is:

```text
sample_data_rows=100
sample_data_min=18
sample_data_max=75
```

### Q2 Required Functions

- `parse_config(filepath: str) -> dict` - Parse key=value config file into dictionary
- `validate_config(config: dict) -> dict` - Validate config values with if/elif/else logic
  - Rules: sample_data_rows must be an int and > 0, sample_data_min must be an int and >= 1, sample_data_max must be an int and > sample_data_min
  - Returns: {key: True/False} for each validation rule
- `generate_sample_data(filename: str, config: dict) -> None` - Generate file with random numbers using config parameters
- `calculate_statistics(data: list) -> dict` - Calculate mean, median, sum, count from list of numbers

### Q2 Required Outputs

- `data/sample_data.csv` - Generated file with 100 random numbers (18-75), one per row
- `output/statistics.txt` - Statistics calculated from generated sample data

**Key concepts:** File I/O, string parsing, conditional logic, basic statistics

## Question 3: Data Utilities Library (20 points)

File: `q3_data_utils.py`

Create a reusable library of pandas functions. These will be imported and used in Q4-Q7 notebooks.

### Q3 Required Functions

- `load_data(filepath: str) -> pd.DataFrame` - Load CSV file
- `clean_data(df, remove_duplicates=True, sentinel_value=-999) -> pd.DataFrame` - Remove duplicates, replace sentinel values
- `detect_missing(df) -> pd.Series` - Count missing values per column
- `fill_missing(df, column, strategy='mean') -> pd.DataFrame` - Fill missing values (mean/median/ffill)
- `filter_data(df, filters: list) -> pd.DataFrame` - Apply multiple filters (equals, greater_than, in_range, etc.)
- `transform_types(df, type_map: dict) -> pd.DataFrame` - Convert column types (datetime, numeric, category)
- `create_bins(df, column, bins, labels, new_column=None) -> pd.DataFrame` - Create categorical bins using pd.cut() (new_column parameter allows custom naming)
- `summarize_by_group(df, group_col, agg_dict=None) -> pd.DataFrame` - Group and aggregate data (agg_dict parameter allows custom aggregations)

**Key concepts:** Data loading, cleaning, missing data handling, filtering, type conversion, binning, grouping


## Question 4: Data Exploration (15 points)

File: `q4_exploration.ipynb`

Complete the notebook to explore the clinical trial data using your Q3 utilities.

### Q4 Tasks

1. **Load and inspect data**
   - Import your `q3_data_utils` module
   - Load `data/clinical_trial_raw.csv`
   - Display basic info (shape, dtypes, first few rows)

2. **Generate site distribution**
   - Calculate value counts for the 'site' column
   - Save to `output/q4_site_counts.csv`

3. **Explore numeric variables**
   - Display summary statistics for numeric columns
   - Identify columns with outliers

4. **Categorical analysis**
   - Calculate value counts for intervention groups
   - Calculate value counts for sex distribution

### Q4 Required Output

- `output/q4_site_counts.csv`


## Question 5: Missing Data Analysis (15 points)

File: `q5_missing_data.ipynb`

Complete the notebook to analyze and handle missing data.

### Q5 Tasks

1. **Detect missing data**
   - Use your utility function to find missing values
   - Display counts and percentages

2. **Apply filling strategies**
   - Fill numeric columns with appropriate methods (mean/median)
   - Use forward fill for time-series columns
   - Document your choices

3. **Handle critical missing values**
   - Drop rows with missing values in critical columns (patient_id, outcome variables)

4. **Save cleaned data**
   - Save to `output/q5_cleaned_data.csv`
   - Generate missing data report to `output/q5_missing_report.txt`

### Q5 Required Outputs

- `output/q5_cleaned_data.csv`
- `output/q5_missing_report.txt`


## Question 6: Data Transformation (20 points)

File: `q6_transformation.ipynb`

Complete the notebook to transform and engineer features.

### Q6 Tasks

1. **Type conversions**
   - Convert enrollment_date to datetime
   - Convert categorical columns to category dtype
   - Convert string columns that should be numeric to numeric type

2. **Feature engineering**
   - Create cholesterol ratio (LDL/HDL)
   - Create age groups using bins: [0, 40, 55, 70, 100] → ['<40', '40-54', '55-69', '70+']
   - Create BMI categories: [0, 18.5, 25, 30, 100] → ['Underweight', 'Normal', 'Overweight', 'Obese']

3. **Clean and encode**
   - Remove any remaining duplicate rows
   - Create dummy variables for intervention_group

4. **Save transformed data**
   - Save to `output/q6_transformed_data.csv`

### Q6 Required Output

- `output/q6_transformed_data.csv`


## Question 7: Aggregation & Analysis (15 points)

File: `q7_aggregation.ipynb`

Complete the notebook to perform grouped analysis.

### Q7 Tasks

1. **Site-level summary**
   - Group by site
   - Calculate mean age, BMI, and patient count per site
   - Save to `output/q7_site_summary.csv`

2. **Intervention comparison**
   - Group by intervention_group
   - Compare outcome rates, adverse events, adherence
   - Save to `output/q7_intervention_comparison.csv`

3. **Advanced analysis**
   - Find top 10 patients by cholesterol_total
   - Calculate statistics by age_group (created in Q6)

4. **Generate report**
   - Save key findings to `output/q7_analysis_report.txt`

### Q7 Required Outputs

- `output/q7_site_summary.csv`
- `output/q7_intervention_comparison.csv`
- `output/q7_analysis_report.txt`


## Question 8: Pipeline Automation Script (5 points)

File: `q8_run_pipeline.sh`

Create an executable shell script that runs the entire analysis pipeline.

### Q8 Requirements

Your script must:

1. **Be executable** (`chmod +x q8_run_pipeline.sh`)
2. **Have shebang**
3. **Run Q4-Q7 notebooks** - Execute notebooks in order using `jupyter nbconvert --execute --to notebook`
4. **Handle errors gracefully** - Use `||` operator to check exit codes and stop on failure
5. **Generate pipeline log** - Save execution status to `reports/pipeline_log.txt`

**Note:** This script assumes Q1 has already been run to create directories and generate the dataset.

### Q8 Tasks

Your script should:

1. Run the Q4-Q7 notebooks in order using jupyter nbconvert
2. NOTE: We assume Q1 has already been run to create directories and generate the dataset
3. NOTE: Q2 is a standalone Python fundamentals exercise, not part of the main pipeline
4. NOTE: Q3 script is not run directly, instead functions from it are imported in Q4-Q7
5. Handle errors gracefully (stop on failure)
6. Log all execution steps to the pipeline log

See the scaffold for error handling examples using the `||` operator.

### Q8 Required Output

- `reports/pipeline_log.txt` - Pipeline execution log with success/error messages


## Testing

Your assignment will be tested using:

1. **Artifact-based testing** - Check for required files and outputs
2. **Pipeline execution** - Run scripts and notebooks in order with error handling
3. **Function testing** - Import Q3 utilities and test with sample data
4. **Output validation** - Verify files exist and contain expected data

See `GRADING_SPEC.md` for detailed testing criteria and point breakdowns.


## Tips

See `TIPS.md` for:

- Common pandas patterns
- Debugging strategies

Good luck!
