{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c872480",
   "metadata": {},
   "source": [
    "# Question 2: Resampling and Frequency Conversion\n",
    "\n",
    "This question focuses on resampling operations and frequency conversion using ICU monitoring data (hourly) and patient vital signs data (daily).\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc29c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"default\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1eeada",
   "metadata": {},
   "source": [
    "## Part 2.1: Load and Prepare Data\n",
    "\n",
    "**Note:** These datasets have realistic characteristics:\n",
    "- **ICU Monitoring**: 75 patients with variable stay lengths (2-30 days). Not all patients are present for the entire 6-month period - patients are admitted and discharged at different times.\n",
    "- **Patient Vitals**: Already contains some missing visits (~5% missing data). This is realistic and will be useful for practicing missing data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb041271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ICU monitoring data (hourly)\n",
    "icu_monitoring = pd.read_csv(\"data/icu_monitoring.csv\")\n",
    "\n",
    "# Load patient vitals data (daily) - for comparison\n",
    "patient_vitals = pd.read_csv(\"data/patient_vitals.csv\")\n",
    "\n",
    "print(\"ICU monitoring shape:\", icu_monitoring.shape)\n",
    "print(\"Patient vitals shape:\", patient_vitals.shape)\n",
    "\n",
    "# Convert datetime columns and set as index\n",
    "icu_monitoring[\"datetime\"] = pd.to_datetime(icu_monitoring[\"datetime\"])\n",
    "icu_monitoring = icu_monitoring.set_index(\"datetime\")\n",
    "\n",
    "patient_vitals[\"date\"] = pd.to_datetime(patient_vitals[\"date\"])\n",
    "patient_vitals = patient_vitals.set_index(\"date\")\n",
    "\n",
    "print(\"\\nICU monitoring sample:\")\n",
    "print(icu_monitoring.head())\n",
    "print(\"\\nPatient vitals sample:\")\n",
    "print(patient_vitals.head())\n",
    "\n",
    "# Check data characteristics\n",
    "print(f\"\\nICU patients: {icu_monitoring['patient_id'].nunique()}\")\n",
    "print(\n",
    "    f\"ICU date range: {icu_monitoring.index.min()} to {icu_monitoring.index.max()}\"\n",
    ")\n",
    "print(f\"\\nPatient vitals patients: {patient_vitals['patient_id'].nunique()}\")\n",
    "print(\n",
    "    f\"Patient vitals date range: {patient_vitals.index.min()} to {patient_vitals.index.max()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37c5ee",
   "metadata": {},
   "source": [
    "## Part 2.2: Time Series Selection\n",
    "\n",
    "**TODO: Perform time series indexing and selection**\n",
    "\n",
    "**Important Note:** Since multiple patients share the same date, the `patient_vitals` index is non-monotonic (not strictly increasing). For reliable date-based selection with `.loc`, you should sort the index first: `patient_vitals = patient_vitals.sort_index()`. This ensures pandas can properly handle date range selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71905eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select data by specific dates\n",
    "# Note: Not all patients may have data on January 1, 2023 (some start later)\n",
    "# Important: Sort the index first since multiple patients share the same date\n",
    "# patient_vitals = patient_vitals.sort_index()  # Sort for reliable date-based selection\n",
    "# january_first = None  # Select January 1, 2023 from patient_vitals\n",
    "# print(\"January 1, 2023 data:\", january_first)\n",
    "# print(f\"Records on Jan 1: {len(january_first)} (some patients may start later)\")\n",
    "\n",
    "# TODO: Select data by date ranges\n",
    "# january_data = None  # Select entire January 2023\n",
    "# print(\"January 2023 shape:\", january_data.shape)\n",
    "\n",
    "# TODO: Select data by time periods\n",
    "# first_quarter = None  # Select Q1 2023\n",
    "# entire_year = None  # Select all of 2023 (will include patients with partial year data)\n",
    "\n",
    "# TODO: Select first and last periods using .loc\n",
    "# first_week = patient_vitals.loc[:patient_vitals.index.min() + pd.Timedelta(days=6)]  # First 7 days\n",
    "# last_week = patient_vitals.loc[patient_vitals.index.max() - pd.Timedelta(days=6):]  # Last 7 days\n",
    "\n",
    "# TODO: Use truncate() method\n",
    "# Note: truncate() requires a sorted index. Sort first if needed: patient_vitals = patient_vitals.sort_index()\n",
    "# data_after_june = None  # Truncate before June 1, 2023\n",
    "# data_before_september = None  # Truncate after August 31, 2023\n",
    "\n",
    "# TODO: Use selected data for analysis\n",
    "# Compare average temperature between first quarter and data after June\n",
    "# print(f\"\\nFirst quarter average temperature: {first_quarter['temperature'].mean():.2f}°F\")\n",
    "# print(f\"After June average temperature: {data_after_june['temperature'].mean():.2f}°F\")\n",
    "# print(f\"First week average temperature: {first_week['temperature'].mean():.2f}°F\")\n",
    "# print(f\"Last week average temperature: {last_week['temperature'].mean():.2f}°F\")\n",
    "\n",
    "# For ICU data with time components:\n",
    "# TODO: Select business hours (9 AM to 5 PM)\n",
    "# business_hours = None  # Use between_time()\n",
    "# print(\"Business hours data shape:\", business_hours.shape)\n",
    "\n",
    "# TODO: Select specific time (noon readings)\n",
    "# noon_data = None  # Use at_time('12:00')\n",
    "\n",
    "# TODO: Use time-based selection for analysis\n",
    "# Compare vital signs during business hours vs other times\n",
    "# all_hours_avg = icu_monitoring.select_dtypes(include=[np.number]).mean()\n",
    "# business_hours_avg = business_hours.select_dtypes(include=[np.number]).mean()\n",
    "# print(f\"\\nAverage heart rate - All hours: {all_hours_avg['heart_rate']:.1f} bpm\")\n",
    "# print(f\"Average heart rate - Business hours: {business_hours_avg['heart_rate']:.1f} bpm\")\n",
    "# print(f\"Average temperature - All hours: {all_hours_avg['temperature']:.1f}°F\")\n",
    "# print(f\"Average temperature - Business hours: {business_hours_avg['temperature']:.1f}°F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189ff1d",
   "metadata": {},
   "source": [
    "## Part 2.3: Resampling Operations\n",
    "\n",
    "**TODO: Perform resampling and frequency conversion**\n",
    "\n",
    "**Important Note:** When resampling DataFrames that contain non-numeric columns (like `patient_id`), you'll get an error if you try to aggregate them with numeric functions like `mean()`. Use `df.select_dtypes(include=[np.number])` to select only numeric columns before resampling, or specify which columns to aggregate in `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Resample hourly ICU data to daily\n",
    "# Note: Exclude non-numeric columns like 'patient_id' when resampling\n",
    "# Select only numeric columns before resampling\n",
    "# numeric_cols = icu_monitoring.select_dtypes(include=[np.number]).columns\n",
    "# icu_daily = icu_monitoring[numeric_cols].resample('D').mean()\n",
    "# print(\"ICU daily shape:\", icu_daily.shape)\n",
    "\n",
    "# TODO: Resample daily patient data to weekly\n",
    "# Note: Exclude 'patient_id' column when resampling\n",
    "# Select only numeric columns before resampling\n",
    "# numeric_cols_pv = patient_vitals.select_dtypes(include=[np.number]).columns\n",
    "# patient_vitals_weekly = patient_vitals[numeric_cols_pv].resample('W').mean()\n",
    "# print(\"Weekly resampled shape:\", patient_vitals_weekly.shape)\n",
    "\n",
    "# TODO: Resample daily patient data to monthly\n",
    "# patient_vitals_monthly = None  # Resample to monthly with mean aggregation (use freq='ME' for Month End)\n",
    "# print(\"Monthly resampled shape:\", patient_vitals_monthly.shape)\n",
    "\n",
    "# TODO: Use different aggregation functions (mean, sum, max, min)\n",
    "# icu_daily_stats = None  # Resample with multiple aggregations\n",
    "# Example: resample('D').agg({'heart_rate': ['mean', 'max', 'min'],\n",
    "#                             'temperature': 'mean'})\n",
    "\n",
    "# TODO: Handle missing values during resampling\n",
    "# Demonstrate upsampling (monthly to daily) creates missing values\n",
    "# Note: When upsampling, use .asfreq() to create missing values, or use .resample() with aggregation\n",
    "# monthly_to_daily = None  # Upsample monthly data to daily (use .asfreq() or .resample('D'))\n",
    "# print(\"Missing values after upsampling:\", monthly_to_daily.isna().sum())\n",
    "\n",
    "# TODO: Compare different resampling frequencies\n",
    "# Create a DataFrame comparing resampling results at different frequencies\n",
    "# Important: Since patient_vitals contains multiple patients per date, you need to aggregate by date first\n",
    "# to create a single daily time series for comparison.\n",
    "# Why aggregation is needed: The patient_vitals DataFrame has multiple rows per date (one for each patient),\n",
    "# so we need to average across patients for each date to create a single daily time series that can be\n",
    "# meaningfully compared with the weekly and monthly resampled data. Without aggregation, resampling would\n",
    "# operate on each patient's time series separately, making it difficult to compare frequencies meaningfully.\n",
    "# Steps:\n",
    "# 1. Since 'date' is currently the index, reset it to a column first, then aggregate by date\n",
    "#    Note: groupby('date').mean() automatically sets 'date' as the index in the result, so you don't need\n",
    "#    to call set_index('date') again after groupby.\n",
    "#    patient_vitals_reset = patient_vitals[numeric_cols_pv].reset_index()\n",
    "#    patient_vitals_daily_agg = patient_vitals_reset.groupby('date').mean()\n",
    "#    # The date is already the index after groupby, so no need to set_index again\n",
    "# 2. Compare the aggregated daily data with weekly and monthly resampled data\n",
    "# Use patient_vitals data resampled to different frequencies:\n",
    "# - Original daily data (aggregated by date): patient_vitals_daily_agg\n",
    "# - Weekly resampled (patient_vitals_weekly)\n",
    "# - Monthly resampled (patient_vitals_monthly)\n",
    "# Include columns: frequency, date_range, row_count, mean_temperature, std_temperature\n",
    "# Use the 'temperature' column from each resampled dataset\n",
    "# Example structure:\n",
    "# resampling_comparison = pd.DataFrame({\n",
    "#     'frequency': ['daily', 'weekly', 'monthly'],\n",
    "#     'date_range': [...],  # Use index.min() and index.max() for each dataset\n",
    "#     'row_count': [...],  # Use len() for each dataset\n",
    "#     'mean_temperature': [...],  # Use .mean() on 'temperature' column for each dataset\n",
    "#     'std_temperature': [...]   # Use .std() on 'temperature' column for each dataset\n",
    "# })\n",
    "\n",
    "# TODO: Save results as 'output/q2_resampling_analysis.csv'\n",
    "# resampling_comparison.to_csv('output/q2_resampling_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e926ee",
   "metadata": {},
   "source": [
    "## Part 2.4: Missing Data Handling\n",
    "\n",
    "**Note:** We'll use upsampling to create missing values for practice. The patient_vitals dataset also contains some naturally occurring missing data (~5% missing visits), which you could use as an alternative approach in practice.\n",
    "\n",
    "**Important:** See the \"Missing Data Handling\" section in `assignment/README.md` for detailed guidance on creating time series with missing values and choosing imputation methods.\n",
    "\n",
    "**TODO: Handle missing data in time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify missing values in time series\n",
    "# Recommended approach: Create missing values by upsampling (e.g., monthly to daily)\n",
    "#   - Use the monthly resampled data from Part 2.3: patient_vitals_monthly['temperature']\n",
    "#   - Upsample to daily frequency using .resample('D').asfreq()\n",
    "#   - This creates missing values for all days except month-end dates\n",
    "# Alternative approach (for practice): You could also use naturally occurring missing data from patient_vitals\n",
    "#   by aggregating by date and using groupby('date'), but upsampling provides a clearer pattern for demonstration\n",
    "# ts_with_missing = None  # Time series with missing values\n",
    "# print(\"Missing value count:\", ts_with_missing.isna().sum())\n",
    "# print(\"Missing value percentage:\", ts_with_missing.isna().sum() / len(ts_with_missing) * 100)\n",
    "\n",
    "# TODO: Use forward fill and backward fill\n",
    "# ts_ffill = None  # Forward fill missing values (use .ffill() method)\n",
    "# ts_bfill = None  # Backward fill missing values (use .bfill() method)\n",
    "\n",
    "# TODO: Use interpolation methods\n",
    "# ts_interpolated = None  # Interpolate missing values\n",
    "# ts_interpolated_linear = None  # Linear interpolation\n",
    "# ts_interpolated_time = None  # Time-based interpolation\n",
    "\n",
    "# TODO: Use rolling mean for imputation\n",
    "# ts_rolling_imputed = None  # Fill missing with rolling mean\n",
    "\n",
    "# TODO: Create missing data report\n",
    "# Document your missing data handling with the following sections:\n",
    "# 1. Missing value summary: Total count and percentage\n",
    "# 2. Missing data patterns: When/why data is missing (by month, day of week, etc.)\n",
    "# 3. Imputation method: Which method you used (forward fill, backward fill, interpolation, rolling mean)\n",
    "# 4. Rationale: Why you chose that method\n",
    "# 5. Pros and cons: Advantages and limitations of your approach\n",
    "# 6. Example: Show at least one example of missing data before and after imputation\n",
    "# Minimum length: 300 words\n",
    "missing_data_report = \"\"\"\n",
    "TODO: Document your missing data handling:\n",
    "- How many missing values did you find?\n",
    "- What percentage of data was missing?\n",
    "- Which method did you use to fill missing values?\n",
    "- Why did you choose that method?\n",
    "- What are the pros/cons of your approach?\n",
    "- Include examples showing missing data patterns\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Document missing data patterns\n",
    "# Analyze when/why data is missing\n",
    "# missing_by_month = ts_with_missing.groupby(ts_with_missing.index.month).apply(lambda x: x.isna().sum())\n",
    "# missing_by_day = ts_with_missing.groupby(ts_with_missing.index.dayofweek).apply(lambda x: x.isna().sum())\n",
    "# missing_patterns = f\"Missing by month:\\n{missing_by_month}\\n\\nMissing by day of week:\\n{missing_by_day}\"\n",
    "\n",
    "# TODO: Save results as 'output/q2_missing_data_report.txt'\n",
    "# with open('output/q2_missing_data_report.txt', 'w') as f:\n",
    "#     f.write(missing_data_report)\n",
    "#     f.write(f\"\\n\\nMissing patterns:\\n{missing_patterns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed28c7",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before moving to Question 3, verify you've created:\n",
    "\n",
    "- [ ] `output/q2_resampling_analysis.csv` - resampling analysis results\n",
    "- [ ] `output/q2_missing_data_report.txt` - missing data handling report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
