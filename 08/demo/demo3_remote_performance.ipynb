{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b0b7a0",
   "metadata": {},
   "source": [
    "# Demo 3: Remote Computing and Performance\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up SSH connections for remote computing\n",
    "- Use tmux for persistent sessions\n",
    "- Optimize performance for large datasets\n",
    "- Apply parallel processing techniques\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set inline plotting for Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd600b2",
   "metadata": {},
   "source": [
    "## Part 1: Performance Optimization\n",
    "\n",
    "### Create Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large dataset for performance testing\n",
    "print(\"=== Creating Large Dataset ===\")\n",
    "n_rows = 100000\n",
    "n_groups = 1000\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    'group': np.random.randint(0, n_groups, n_rows),\n",
    "    'value1': np.random.randn(n_rows),\n",
    "    'value2': np.random.randn(n_rows),\n",
    "    'value3': np.random.randn(n_rows),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_rows)\n",
    "}\n",
    "\n",
    "df_large = pd.DataFrame(data)\n",
    "print(f\"Dataset shape: {df_large.shape}\")\n",
    "print(f\"Memory usage: {df_large.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea6209",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06455191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different aggregation methods\n",
    "print(\"=== Performance Comparison ===\")\n",
    "\n",
    "# Method 1: Multiple groupby operations\n",
    "start_time = time.time()\n",
    "result1 = df_large.groupby('group')['value1'].sum()\n",
    "result2 = df_large.groupby('group')['value2'].sum()\n",
    "result3 = df_large.groupby('group')['value3'].sum()\n",
    "method1_time = time.time() - start_time\n",
    "\n",
    "# Method 2: Single groupby with multiple aggregations\n",
    "start_time = time.time()\n",
    "result4 = df_large.groupby('group').agg({\n",
    "    'value1': 'sum',\n",
    "    'value2': 'sum',\n",
    "    'value3': 'sum'\n",
    "})\n",
    "method2_time = time.time() - start_time\n",
    "\n",
    "print(f\"Method 1 (multiple groupby): {method1_time:.4f} seconds\")\n",
    "print(f\"Method 2 (single groupby): {method2_time:.4f} seconds\")\n",
    "print(f\"Performance improvement: {method1_time/method2_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334136e",
   "metadata": {},
   "source": [
    "### Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a44c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization techniques\n",
    "print(\"=== Memory Optimization ===\")\n",
    "\n",
    "# Check data types\n",
    "print(\"Original data types:\")\n",
    "print(df_large.dtypes)\n",
    "print(f\"Memory usage: {df_large.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Optimize data types\n",
    "df_optimized = df_large.copy()\n",
    "df_optimized['group'] = df_optimized['group'].astype('category')\n",
    "df_optimized['category'] = df_optimized['category'].astype('category')\n",
    "\n",
    "print(\"\\nOptimized data types:\")\n",
    "print(df_optimized.dtypes)\n",
    "print(f\"Memory usage: {df_optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - df_optimized.memory_usage(deep=True).sum() / df_large.memory_usage(deep=True).sum()) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd8fc1",
   "metadata": {},
   "source": [
    "## Part 2: Parallel Processing\n",
    "\n",
    "### Chunked Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eae264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data in chunks\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Process a single chunk of data\"\"\"\n",
    "    return chunk.groupby('group').agg({\n",
    "        'value1': 'sum',\n",
    "        'value2': 'sum',\n",
    "        'value3': 'sum'\n",
    "    })\n",
    "\n",
    "# Sequential processing\n",
    "print(\"=== Sequential Processing ===\")\n",
    "start_time = time.time()\n",
    "chunk_size = 10000\n",
    "chunks = [df_large.iloc[i:i+chunk_size] for i in range(0, len(df_large), chunk_size)]\n",
    "sequential_results = []\n",
    "for chunk in chunks:\n",
    "    result = process_chunk(chunk)\n",
    "    sequential_results.append(result)\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"Sequential processing time: {sequential_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0348df2",
   "metadata": {},
   "source": [
    "### Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing (simplified for demo)\n",
    "print(\"=== Parallel Processing ===\")\n",
    "print(\"Note: Multiprocessing requires proper __main__ guard in production code\")\n",
    "print(\"For this demo, we'll simulate parallel processing benefits:\")\n",
    "print(f\"Sequential processing time: {sequential_time:.4f} seconds\")\n",
    "print(f\"Estimated parallel processing time: {sequential_time/4:.4f} seconds\")\n",
    "print(f\"Estimated speedup: 4.0x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798256f",
   "metadata": {},
   "source": [
    "## Part 3: Remote Computing Simulation\n",
    "\n",
    "### SSH Connection Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate SSH connection setup\n",
    "print(\"=== SSH Connection Simulation ===\")\n",
    "print(\"In a real scenario, you would:\")\n",
    "print(\"1. Generate SSH key pair: ssh-keygen -t rsa -b 4096\")\n",
    "print(\"2. Copy public key to server: ssh-copy-id username@server.com\")\n",
    "print(\"3. Connect to server: ssh username@server.com\")\n",
    "print(\"4. Set up environment on remote server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac2493",
   "metadata": {},
   "source": [
    "### tmux Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed08f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate tmux session management\n",
    "print(\"=== tmux Session Management ===\")\n",
    "print(\"tmux commands for persistent sessions:\")\n",
    "print(\"- tmux new-session -s analysis\")\n",
    "print(\"- tmux list-sessions\")\n",
    "print(\"- tmux attach-session -t analysis\")\n",
    "print(\"- Ctrl+b d (detach from session)\")\n",
    "print(\"- tmux kill-session -t analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcb91e",
   "metadata": {},
   "source": [
    "### Remote Data Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7154cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate remote data analysis workflow\n",
    "def simulate_remote_analysis():\n",
    "    \"\"\"Simulate remote data analysis workflow\"\"\"\n",
    "    print(\"=== Remote Data Analysis Workflow ===\")\n",
    "    \n",
    "    # Simulate loading large dataset\n",
    "    print(\"1. Loading large dataset on remote server...\")\n",
    "    time.sleep(0.1)  # Simulate loading time\n",
    "    \n",
    "    # Simulate analysis\n",
    "    print(\"2. Performing aggregation analysis...\")\n",
    "    start_time = time.time()\n",
    "    result = df_large.groupby('group').agg({\n",
    "        'value1': ['sum', 'mean', 'std'],\n",
    "        'value2': ['sum', 'mean', 'std'],\n",
    "        'value3': ['sum', 'mean', 'std']\n",
    "    })\n",
    "    analysis_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"3. Analysis completed in {analysis_time:.4f} seconds\")\n",
    "    print(f\"4. Results shape: {result.shape}\")\n",
    "    \n",
    "    # Simulate saving results\n",
    "    print(\"5. Saving results to remote server...\")\n",
    "    time.sleep(0.1)  # Simulate saving time\n",
    "    \n",
    "    # Simulate downloading results\n",
    "    print(\"6. Downloading results to local machine...\")\n",
    "    time.sleep(0.1)  # Simulate download time\n",
    "    \n",
    "    print(\"7. Remote analysis workflow completed!\")\n",
    "    return result\n",
    "\n",
    "# Run simulation\n",
    "remote_result = simulate_remote_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2361d41",
   "metadata": {},
   "source": [
    "## Part 4: Performance Monitoring\n",
    "\n",
    "### Memory Usage Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor memory usage during operations\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def monitor_memory():\n",
    "    \"\"\"Monitor memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return memory_mb\n",
    "\n",
    "print(\"=== Memory Usage Monitoring ===\")\n",
    "print(f\"Initial memory usage: {monitor_memory():.2f} MB\")\n",
    "\n",
    "# Perform memory-intensive operation\n",
    "start_memory = monitor_memory()\n",
    "large_operation = df_large.groupby('group').agg({\n",
    "    'value1': 'sum',\n",
    "    'value2': 'sum',\n",
    "    'value3': 'sum'\n",
    "})\n",
    "end_memory = monitor_memory()\n",
    "\n",
    "print(f\"Memory usage after operation: {end_memory:.2f} MB\")\n",
    "print(f\"Memory increase: {end_memory - start_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689ae74",
   "metadata": {},
   "source": [
    "### Performance Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e98f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance profiling\n",
    "def profile_operation(func, *args, **kwargs):\n",
    "    \"\"\"Profile a function's performance\"\"\"\n",
    "    start_time = time.time()\n",
    "    start_memory = monitor_memory()\n",
    "    \n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_memory = monitor_memory()\n",
    "    \n",
    "    return {\n",
    "        'result': result,\n",
    "        'execution_time': end_time - start_time,\n",
    "        'memory_usage': end_memory - start_memory\n",
    "    }\n",
    "\n",
    "# Profile different operations\n",
    "print(\"=== Performance Profiling ===\")\n",
    "\n",
    "# Profile groupby operation\n",
    "groupby_profile = profile_operation(\n",
    "    lambda: df_large.groupby('group')['value1'].sum()\n",
    ")\n",
    "print(f\"GroupBy operation: {groupby_profile['execution_time']:.4f}s, {groupby_profile['memory_usage']:.2f}MB\")\n",
    "\n",
    "# Profile pivot table operation\n",
    "pivot_profile = profile_operation(\n",
    "    lambda: pd.pivot_table(df_large, values='value1', index='group', columns='category', aggfunc='sum')\n",
    ")\n",
    "print(f\"Pivot table operation: {pivot_profile['execution_time']:.4f}s, {pivot_profile['memory_usage']:.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7c1d5",
   "metadata": {},
   "source": [
    "## Part 5: Optimization Strategies\n",
    "\n",
    "### Data Type Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize data types for better performance\n",
    "print(\"=== Data Type Optimization ===\")\n",
    "\n",
    "# Check current data types\n",
    "print(\"Current data types:\")\n",
    "print(df_large.dtypes)\n",
    "\n",
    "# Optimize integer columns\n",
    "df_optimized = df_large.copy()\n",
    "df_optimized['group'] = pd.Categorical(df_optimized['group'])\n",
    "\n",
    "# Measure performance improvement\n",
    "start_time = time.time()\n",
    "result_optimized = df_optimized.groupby('group')['value1'].sum()\n",
    "optimized_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "result_original = df_large.groupby('group')['value1'].sum()\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "print(f\"Original performance: {original_time:.4f} seconds\")\n",
    "print(f\"Optimized performance: {optimized_time:.4f} seconds\")\n",
    "print(f\"Performance improvement: {original_time/optimized_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f86017",
   "metadata": {},
   "source": [
    "### Chunked Processing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunked processing for large datasets\n",
    "def process_large_dataset(df, chunk_size=10000):\n",
    "    \"\"\"Process large dataset in chunks\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        chunk_result = chunk.groupby('group').agg({\n",
    "            'value1': 'sum',\n",
    "            'value2': 'sum',\n",
    "            'value3': 'sum'\n",
    "        })\n",
    "        results.append(chunk_result)\n",
    "    \n",
    "    # Combine results\n",
    "    final_result = pd.concat(results).groupby(level=0).sum()\n",
    "    return final_result\n",
    "\n",
    "print(\"=== Chunked Processing Strategy ===\")\n",
    "start_time = time.time()\n",
    "chunked_result = process_large_dataset(df_large, chunk_size=5000)\n",
    "chunked_time = time.time() - start_time\n",
    "\n",
    "print(f\"Chunked processing time: {chunked_time:.4f} seconds\")\n",
    "print(f\"Result shape: {chunked_result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28061421",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Performance Optimization**: Use single groupby with multiple aggregations\n",
    "2. **Memory Optimization**: Optimize data types and use categorical data\n",
    "3. **Parallel Processing**: Use multiprocessing for CPU-intensive tasks\n",
    "4. **Remote Computing**: Use SSH and tmux for large dataset analysis\n",
    "5. **Chunked Processing**: Process large datasets in manageable chunks\n",
    "6. **Performance Monitoring**: Track memory usage and execution time\n",
    "7. **Data Type Optimization**: Use appropriate data types for better performance\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Practice with your own large datasets\n",
    "- Set up remote computing environment\n",
    "- Learn about distributed computing frameworks\n",
    "- Explore cloud computing options for big data analysis"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
