{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd1c140",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Live Demo Guide: Data Analysis and Debugging Techniques\n",
    "\n",
    "**Duration:** 30-35 minutes  \n",
    "**Format:** Interactive coding demonstration with student participation\n",
    "\n",
    "## Demo Overview\n",
    "\n",
    "Show students professional data analysis workflows through systematic exploration of a real dataset, demonstrating quality assessment, debugging techniques, and robust analysis patterns they'll use in industry.\n",
    "\n",
    "### Learning Objectives\n",
    "Students will observe:\n",
    "- Systematic approach to unknown data exploration\n",
    "- Professional data quality assessment workflow\n",
    "- Debugging techniques for data analysis code\n",
    "- Validation and assumption checking patterns\n",
    "- Professional documentation and reporting practices\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-Demo Setup (5 minutes before class)\n",
    "\n",
    "### 1. Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a5f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Activate environment and install any missing packages\n",
    "conda activate datasci217\n",
    "pip install tabulate memory-profiler\n",
    "\n",
    "# Create demo directory\n",
    "mkdir -p demo_analysis\n",
    "cd demo_analysis\n",
    "\n",
    "# Download demo dataset (or prepare sample data)\n",
    "# Use a dataset with known quality issues for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a224c7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 2. Dataset Preparation\n",
    "Create `messy_sales_data.csv` with intentional issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample dataset with quality issues\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    'customer_id': ['CUST_' + str(i) for i in range(1, 501)] + [None] * 20 + ['CUST_' + str(i) for i in range(1, 21)],  # Duplicates + missing\n",
    "    'product_name': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', None, ' Tablet ', 'laptop'], 541),\n",
    "    'revenue': np.concatenate([np.random.normal(1000, 300, 500), [None] * 20, np.random.normal(5000, 1000, 21)]),\n",
    "    'order_date': pd.date_range('2023-01-01', periods=541, freq='D').astype(str).tolist()[:520] + [None] * 21,\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West', 'north', None], 541)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Add some extreme outliers\n",
    "df.loc[df.index[-5:], 'revenue'] = [50000, 75000, 100000, -1000, 200000]\n",
    "\n",
    "df.to_csv('messy_sales_data.csv', index=False)\n",
    "print(\"Demo dataset created with intentional quality issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4303bbd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 3. Code Templates\n",
    "Prepare `demo_analysis.py` skeleton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_dataset(df, target_column=None):\n",
    "    \"\"\"Template function for systematic analysis\"\"\"\n",
    "    pass\n",
    "\n",
    "def comprehensive_quality_assessment(df):\n",
    "    \"\"\"Template for quality assessment\"\"\"\n",
    "    pass\n",
    "\n",
    "def debug_analysis_step(func, data, step_name):\n",
    "    \"\"\"Template for debugging wrapper\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4fcc3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Live Demo Script\n",
    "\n",
    "### Opening (2 minutes)\n",
    "\n",
    "> **Instructor:** \"Today we're going to analyze a real dataset the way you would in industry - we don't know what we'll find, but we'll use systematic approaches to understand the data and handle any issues professionally. \n",
    ">\n",
    "> I'm going to load this sales dataset and we'll discover together what challenges it presents. This mirrors what you'll encounter in real data science work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76890ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mysterious dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('messy_sales_data.csv')\n",
    "\n",
    "print(f\"Initial look: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8dcef",
   "metadata": {},
   "source": [
    "**Ask students:** \"What should we check first before doing any analysis?\"\n",
    "\n",
    "### Step 1: Systematic Data Exploration (8 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f510b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset_systematically(df):\n",
    "    \"\"\"\n",
    "    Systematic dataset exploration - live coding\n",
    "    \"\"\"\n",
    "    print(\"=== SYSTEMATIC DATA EXPLORATION ===\")\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # First look at data\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check for obvious issues\n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run exploration\n",
    "df_info = explore_dataset_systematically(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161de82d",
   "metadata": {},
   "source": [
    "**Pause for student questions:** \"What do you notice about this data? Any red flags?\"\n",
    "\n",
    "**Expected student observations:**\n",
    "- Missing values in multiple columns\n",
    "- Different case variations in text\n",
    "- Some very high revenue values\n",
    "- Duplicates in customer_id\n",
    "\n",
    "### Step 2: Quality Assessment - Live Problem Solving (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_quality_assessment(df):\n",
    "    \"\"\"\n",
    "    Quality assessment with live debugging\n",
    "    \"\"\"\n",
    "    print(\"=== LIVE QUALITY ASSESSMENT ===\")\n",
    "    \n",
    "    quality_issues = {}\n",
    "    \n",
    "    # 1. Missing values analysis\n",
    "    print(\"\\n1. MISSING VALUES:\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    \n",
    "    for col, count in missing_data[missing_data > 0].items():\n",
    "        print(f\"  {col}: {count} missing ({missing_percent[col]:.1f}%)\")\n",
    "        quality_issues[f'{col}_missing'] = count\n",
    "    \n",
    "    # 2. Duplicate analysis - demonstrate debugging\n",
    "    print(\"\\n2. DUPLICATE ANALYSIS:\")\n",
    "    total_duplicates = df.duplicated().sum()\n",
    "    print(f\"Total duplicate rows: {total_duplicates}\")\n",
    "    \n",
    "    # Let's investigate customer_id duplicates specifically\n",
    "    customer_duplicates = df['customer_id'].duplicated().sum()\n",
    "    print(f\"Customer ID duplicates: {customer_duplicates}\")\n",
    "    \n",
    "    # Show the duplicates - this is where students learn debugging\n",
    "    if customer_duplicates > 0:\n",
    "        print(\"Let's examine these duplicates:\")\n",
    "        duplicate_customers = df[df['customer_id'].duplicated(keep=False) & df['customer_id'].notna()]\n",
    "        print(duplicate_customers[['customer_id', 'product_name', 'revenue']].head(10))\n",
    "    \n",
    "    # 3. Data consistency issues\n",
    "    print(\"\\n3. CONSISTENCY ISSUES:\")\n",
    "    \n",
    "    # Product names - demonstrate string cleaning needs\n",
    "    print(\"Product name variations:\")\n",
    "    print(df['product_name'].value_counts())\n",
    "    \n",
    "    # Revenue outliers - demonstrate statistical detection\n",
    "    print(\"\\n4. OUTLIER DETECTION:\")\n",
    "    Q1 = df['revenue'].quantile(0.25)\n",
    "    Q3 = df['revenue'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df['revenue'] < lower_bound) | (df['revenue'] > upper_bound)]\n",
    "    print(f\"Revenue outliers: {len(outliers)} values outside [{lower_bound:.0f}, {upper_bound:.0f}]\")\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        print(\"Extreme values:\")\n",
    "        print(outliers[['customer_id', 'product_name', 'revenue']].head())\n",
    "    \n",
    "    return quality_issues\n",
    "\n",
    "# Run assessment\n",
    "issues = live_quality_assessment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d7f4c",
   "metadata": {},
   "source": [
    "**Interactive moment:** \"Let's vote - which issues should we prioritize fixing first?\"\n",
    "\n",
    "### Step 3: Debugging Analysis Code (8 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_debugging_techniques(df):\n",
    "    \"\"\"\n",
    "    Show professional debugging approaches\n",
    "    \"\"\"\n",
    "    print(\"=== DEBUGGING TECHNIQUES DEMONSTRATION ===\")\n",
    "    \n",
    "    # 1. Validation wrapper\n",
    "    def validate_data_assumptions(df, step_name):\n",
    "        print(f\"\\nValidating assumptions for: {step_name}\")\n",
    "        \n",
    "        checks = {\n",
    "            'has_data': len(df) > 0,\n",
    "            'has_revenue': 'revenue' in df.columns,\n",
    "            'revenue_is_numeric': df['revenue'].dtype in ['int64', 'float64'],\n",
    "            'has_non_null_revenue': df['revenue'].notna().any()\n",
    "        }\n",
    "        \n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"  {status} {check}\")\n",
    "            \n",
    "            if not result:\n",
    "                raise ValueError(f\"Assumption failed: {check}\")\n",
    "    \n",
    "    # 2. Analysis with debugging\n",
    "    def calculate_revenue_by_product_safe(df):\n",
    "        \"\"\"Example analysis with built-in debugging\"\"\"\n",
    "        \n",
    "        # Validate assumptions first\n",
    "        validate_data_assumptions(df, \"revenue_by_product\")\n",
    "        \n",
    "        print(\"\\nCalculating revenue by product...\")\n",
    "        \n",
    "        # Debug: Check data before analysis\n",
    "        print(f\"  Input: {len(df)} rows\")\n",
    "        print(f\"  Non-null revenue: {df['revenue'].notna().sum()}\")\n",
    "        print(f\"  Non-null products: {df['product_name'].notna().sum()}\")\n",
    "        \n",
    "        # Perform analysis with error handling\n",
    "        try:\n",
    "            # Clean data first\n",
    "            clean_data = df[df['revenue'].notna() & df['product_name'].notna()].copy()\n",
    "            print(f\"  After cleaning: {len(clean_data)} rows\")\n",
    "            \n",
    "            # Calculate results\n",
    "            revenue_by_product = clean_data.groupby('product_name')['revenue'].agg(['count', 'mean', 'sum'])\n",
    "            \n",
    "            print(\"  Analysis successful!\")\n",
    "            return revenue_by_product\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Analysis failed: {str(e)}\")\n",
    "            # Return debugging info\n",
    "            return None\n",
    "    \n",
    "    # 3. Demonstrate the debugging in action\n",
    "    try:\n",
    "        results = calculate_revenue_by_product_safe(df)\n",
    "        if results is not None:\n",
    "            print(\"\\nResults:\")\n",
    "            print(results.round(2))\n",
    "    except Exception as e:\n",
    "        print(f\"Caught error: {e}\")\n",
    "        print(\"This is how we handle analysis failures professionally\")\n",
    "\n",
    "# Run debugging demonstration\n",
    "demonstrate_debugging_techniques(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3638397",
   "metadata": {},
   "source": [
    "**Ask students:** \"What did we learn about handling data quality issues during analysis?\"\n",
    "\n",
    "### Step 4: Professional Analysis Pattern (5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_professional_workflow(df):\n",
    "    \"\"\"\n",
    "    Demonstrate complete professional workflow\n",
    "    \"\"\"\n",
    "    print(\"=== PROFESSIONAL ANALYSIS WORKFLOW ===\")\n",
    "    \n",
    "    # Step 1: Data cleaning decisions (document everything)\n",
    "    print(\"\\n1. CLEANING DECISIONS:\")\n",
    "    print(\"  - Remove rows with missing customer_id or revenue\")\n",
    "    print(\"  - Standardize product names (lowercase, strip whitespace)\")\n",
    "    print(\"  - Flag extreme revenue outliers for investigation\")\n",
    "    \n",
    "    # Step 2: Implement cleaning with logging\n",
    "    original_size = len(df)\n",
    "    \n",
    "    # Clean missing values\n",
    "    df_clean = df[df['customer_id'].notna() & df['revenue'].notna()].copy()\n",
    "    print(f\"  After removing missing: {len(df_clean)} rows ({original_size - len(df_clean)} removed)\")\n",
    "    \n",
    "    # Clean product names\n",
    "    df_clean['product_name_clean'] = df_clean['product_name'].str.lower().str.strip()\n",
    "    print(f\"  Product names standardized\")\n",
    "    \n",
    "    # Flag outliers but keep them (business decision)\n",
    "    Q1, Q3 = df_clean['revenue'].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    df_clean['is_outlier'] = (df_clean['revenue'] < Q1 - 1.5*IQR) | (df_clean['revenue'] > Q3 + 1.5*IQR)\n",
    "    print(f\"  Flagged {df_clean['is_outlier'].sum()} outliers\")\n",
    "    \n",
    "    # Step 3: Analysis with validation\n",
    "    print(\"\\n2. ANALYSIS RESULTS:\")\n",
    "    \n",
    "    # Revenue by product (cleaned)\n",
    "    product_analysis = df_clean.groupby('product_name_clean')['revenue'].agg(['count', 'mean', 'sum'])\n",
    "    print(\"Revenue by product (cleaned data):\")\n",
    "    print(product_analysis.round(2))\n",
    "    \n",
    "    # Outlier analysis separately\n",
    "    if df_clean['is_outlier'].any():\n",
    "        print(\"\\nOutlier investigation:\")\n",
    "        outlier_summary = df_clean[df_clean['is_outlier']][['customer_id', 'product_name_clean', 'revenue']]\n",
    "        print(outlier_summary)\n",
    "    \n",
    "    print(\"\\n3. NEXT STEPS:\")\n",
    "    print(\"  - Investigate high-revenue outliers with business team\")\n",
    "    print(\"  - Set up automated quality monitoring\")\n",
    "    print(\"  - Document cleaning decisions for future reference\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Run complete workflow\n",
    "final_data = complete_professional_workflow(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5400e9",
   "metadata": {},
   "source": [
    "### Closing (2 minutes)\n",
    "\n",
    "> **Instructor:** \"Notice how we approached this systematically:\n",
    "> 1. **Explored first** - no assumptions about the data\n",
    "> 2. **Assessed quality** - found issues before analyzing  \n",
    "> 3. **Debugged proactively** - built validation into our analysis\n",
    "> 4. **Documented decisions** - others can understand our choices\n",
    "> 5. **Planned next steps** - analysis is just the beginning\n",
    ">\n",
    "> This is exactly how you'll work with real data in industry. Always expect quality issues, always validate assumptions, and always document your decisions.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Student Participation Moments\n",
    "\n",
    "### Discussion Questions Throughout Demo\n",
    "\n",
    "1. **After initial data load:** \"What should we check first?\"\n",
    "2. **After seeing missing values:** \"How would you prioritize these issues?\"\n",
    "3. **After finding outliers:** \"Should we remove these or investigate them?\"\n",
    "4. **After cleaning decisions:** \"What would happen if we made different choices?\"\n",
    "5. **During debugging:** \"Why is validation important in analysis code?\"\n",
    "\n",
    "### Interactive Coding Moments\n",
    "\n",
    "1. **Student prediction:** Before running quality assessment, have students guess what issues they'll find\n",
    "2. **Student decision:** When finding duplicates, ask class to vote on handling approach\n",
    "3. **Student debugging:** Present a \"broken\" analysis function and ask students to identify the issue\n",
    "4. **Student design:** Ask how they would modify the validation for a different type of dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Common Demo Issues & Solutions\n",
    "\n",
    "### Technical Issues\n",
    "\n",
    "1. **Package not installed:**\n",
    "   ```bash\n",
    "   pip install tabulate memory-profiler\n",
    "   ```\n",
    "\n",
    "2. **Dataset generation fails:**\n",
    "   - Have backup CSV file ready\n",
    "   - Use simpler synthetic data if needed\n",
    "\n",
    "3. **Memory issues with large datasets:**\n",
    "   - Reduce dataset size\n",
    "   - Use `df.head(1000)` for demonstration\n",
    "\n",
    "### Timing Issues\n",
    "\n",
    "**Running ahead of schedule:**\n",
    "- Add more detailed exploration of specific quality issues\n",
    "- Show additional debugging techniques\n",
    "- Demonstrate memory profiling\n",
    "\n",
    "**Running behind schedule:**\n",
    "- Skip detailed outlier investigation\n",
    "- Focus on main concepts: systematic approach, validation, documentation\n",
    "- Save advanced debugging for next class\n",
    "\n",
    "### Student Engagement\n",
    "\n",
    "**Low participation:**\n",
    "- Ask more direct questions: \"Sarah, what do you think this outlier represents?\"\n",
    "- Use polls/votes for decisions\n",
    "- Break into small groups for 2-minute discussions\n",
    "\n",
    "**Too many questions:**\n",
    "- Acknowledge questions but defer complex ones: \"Great question - let's address that in next week's deep dive\"\n",
    "- Keep focus on main workflow pattern\n",
    "\n",
    "---\n",
    "\n",
    "## Post-Demo Activities\n",
    "\n",
    "### Immediate Follow-up (5 minutes)\n",
    "> \"Now you try - I want you to load your assignment dataset and run through the first two steps we just demonstrated. You have 5 minutes to explore and find at least 3 quality issues.\"\n",
    "\n",
    "### Assignment Connection\n",
    "> \"Your assignment this week asks you to build exactly these systems we demonstrated. You'll create your own quality assessment function, build debugging into your analysis, and generate a professional report. Start with the patterns we just showed.\"\n",
    "\n",
    "### Next Class Preview\n",
    "> \"Next week we'll build on this foundation with advanced data manipulation techniques. We'll see how to handle complex data transformations while maintaining the systematic, validated approach you learned today.\""
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
