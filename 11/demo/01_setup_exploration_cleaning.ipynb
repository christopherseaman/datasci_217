{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64123441",
   "metadata": {},
   "source": [
    "# Notebook 1: Setup, Exploration & Cleaning\n",
    "\n",
    "**Phases 1-3:** Project Setup, Data Exploration, and Data Cleaning\n",
    "\n",
    "**Dataset:** NYC Taxi Trip Dataset\n",
    "\n",
    "**Focus:** Getting data ready for analysis - loading, understanding, and cleaning messy real-world data.\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 1: Project Setup & Data Acquisition\n",
    "\n",
    "### Learning Objectives\n",
    "- Set up the analysis environment\n",
    "- Load data from files\n",
    "- Perform initial data inspection\n",
    "- Understand data structure and schema\n",
    "\n",
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter display\n",
    "from IPython.display import display\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c51b2",
   "metadata": {},
   "source": [
    "### Step 2: Load the Data\n",
    "\n",
    "**NYC Taxi Trip Dataset**\n",
    "\n",
    "**Source:** [NYC Taxi & Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "\n",
    "We'll use actual NYC Taxi Trip data downloaded from the NYC TLC website. The data is available in Parquet or CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load actual NYC Taxi Trip data\n",
    "# Download from: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "# NYC TLC provides trip record data in Parquet or CSV format\n",
    "# Place the downloaded file in the data/ directory\n",
    "\n",
    "import os\n",
    "\n",
    "# Check for data file (adjust filename based on what you downloaded)\n",
    "data_files = [\n",
    "    'data/yellow_tripdata_2023-01.parquet',\n",
    "    'data/yellow_tripdata_2023-01.csv',\n",
    "    'data/nyc_taxi_trips.parquet',\n",
    "    'data/nyc_taxi_trips.csv'\n",
    "]\n",
    "\n",
    "data_file = None\n",
    "for file in data_files:\n",
    "    if os.path.exists(file):\n",
    "        data_file = file\n",
    "        break\n",
    "\n",
    "if data_file is None:\n",
    "    print(\"⚠️  NYC Taxi data file not found!\")\n",
    "    print(\"Please download NYC Taxi data from:\")\n",
    "    print(\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\")\n",
    "    print(\"\\nRecommended: Download one month of Yellow Taxi data (Parquet format)\")\n",
    "    print(\"Place it in the data/ directory\")\n",
    "    print(\"\\nExample filenames:\")\n",
    "    print(\"  - data/yellow_tripdata_2023-01.parquet\")\n",
    "    print(\"  - data/yellow_tripdata_2023-01.csv\")\n",
    "    raise FileNotFoundError(\"NYC Taxi data file not found. Please download from NYC TLC website.\")\n",
    "\n",
    "print(f\"Loading NYC Taxi Trip data from: {data_file}\")\n",
    "\n",
    "# Load data - prefer Parquet for better performance\n",
    "if data_file.endswith('.parquet'):\n",
    "    df = pd.read_parquet(data_file)\n",
    "else:\n",
    "    # For CSV, may need to load in chunks if file is large\n",
    "    try:\n",
    "        df = pd.read_csv(data_file, low_memory=False)\n",
    "    except MemoryError:\n",
    "        print(\"File too large, loading in chunks...\")\n",
    "        chunk_list = []\n",
    "        for chunk in pd.read_csv(data_file, chunksize=100000, low_memory=False):\n",
    "            chunk_list.append(chunk)\n",
    "        df = pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Standardize column names for consistency across all notebooks\n",
    "# NYC TLC data uses 'tpep_' prefix for Yellow taxis, 'lpep_' for Green taxis\n",
    "# We'll standardize to simpler names: pickup_datetime, dropoff_datetime, etc.\n",
    "\n",
    "column_mapping = {\n",
    "    # Datetime columns (most important - used throughout)\n",
    "    'tpep_pickup_datetime': 'pickup_datetime',\n",
    "    'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    'lpep_pickup_datetime': 'pickup_datetime',  # Green taxi\n",
    "    'lpep_dropoff_datetime': 'dropoff_datetime',\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "for old_name, new_name in column_mapping.items():\n",
    "    if old_name in df.columns:\n",
    "        df = df.rename(columns={old_name: new_name})\n",
    "\n",
    "# Parse datetime columns immediately (needed for trip_duration calculation)\n",
    "if 'pickup_datetime' in df.columns:\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "if 'dropoff_datetime' in df.columns:\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "\n",
    "# Calculate total_amount if not present (NYC TLC has component columns)\n",
    "if 'total_amount' not in df.columns:\n",
    "    components = ['fare_amount']\n",
    "    if 'tip_amount' in df.columns:\n",
    "        components.append('tip_amount')\n",
    "    if 'tolls_amount' in df.columns:\n",
    "        components.append('tolls_amount')\n",
    "    if 'extra' in df.columns:\n",
    "        components.append('extra')\n",
    "    if 'mta_tax' in df.columns:\n",
    "        components.append('mta_tax')\n",
    "    if 'improvement_surcharge' in df.columns:\n",
    "        components.append('improvement_surcharge')\n",
    "    \n",
    "    if len(components) > 1:\n",
    "        df['total_amount'] = df[components].sum(axis=1)\n",
    "        print(f\"✓ Calculated total_amount from: {', '.join(components)}\")\n",
    "\n",
    "# Verify essential columns exist\n",
    "essential_cols = ['pickup_datetime', 'dropoff_datetime', 'fare_amount', 'trip_distance', 'passenger_count']\n",
    "missing_essential = [col for col in essential_cols if col not in df.columns]\n",
    "\n",
    "if missing_essential:\n",
    "    print(f\"⚠️  Warning: Missing essential columns: {missing_essential}\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    print(\"\\nNote: NYC TLC data structure may vary. Adjust column references as needed.\")\n",
    "else:\n",
    "    print(\"✅ All essential columns found!\")\n",
    "\n",
    "print(f\"\\n✅ Loaded {len(df):,} taxi trips\")\n",
    "print(f\"Columns: {list(df.columns)[:10]}... ({len(df.columns)} total)\")\n",
    "if 'pickup_datetime' in df.columns:\n",
    "    print(f\"Date range: {df['pickup_datetime'].min()} to {df['pickup_datetime'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e2339",
   "metadata": {},
   "source": [
    "### Step 3: Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nShape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(df.memory_usage(deep=True).sum() / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "print(\"=\" * 60)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\" * 60)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9484f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing Percentage': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "display(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2fefc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Data Exploration & Understanding\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand data distributions\n",
    "- Identify relationships between variables\n",
    "- Create initial visualizations\n",
    "- Spot potential data quality issues\n",
    "\n",
    "### Step 1: Basic Statistics and Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5444e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key numeric variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Distribution of Key Variables', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Trip distance\n",
    "axes[0, 0].hist(df['trip_distance'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Trip Distance (miles)')\n",
    "axes[0, 0].set_xlabel('Distance')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Fare amount\n",
    "axes[0, 1].hist(df['fare_amount'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Fare Amount ($)')\n",
    "axes[0, 1].set_xlabel('Fare')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Tip amount (excluding zeros)\n",
    "tips_nonzero = df['tip_amount'].dropna()\n",
    "axes[0, 2].hist(tips_nonzero, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 2].set_title('Tip Amount ($) - Non-zero only')\n",
    "axes[0, 2].set_xlabel('Tip')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# Passenger count\n",
    "passenger_counts = df['passenger_count'].value_counts().sort_index()\n",
    "axes[1, 0].bar(passenger_counts.index, passenger_counts.values, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Passenger Count Distribution')\n",
    "axes[1, 0].set_xlabel('Passengers')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_xticks(passenger_counts.index)\n",
    "\n",
    "# Total amount\n",
    "axes[1, 1].hist(df['total_amount'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_title('Total Amount ($)')\n",
    "axes[1, 1].set_xlabel('Total')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Trip duration (calculate from datetime columns)\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60  # minutes\n",
    "axes[1, 2].hist(df['trip_duration'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 2].set_title('Trip Duration (minutes)')\n",
    "axes[1, 2].set_xlabel('Duration')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478e131",
   "metadata": {},
   "source": [
    "### Step 2: Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4cd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trips over time to see temporal patterns\n",
    "df['pickup_date'] = pd.to_datetime(df['pickup_datetime']).dt.date\n",
    "trips_by_date = df.groupby('pickup_date').size()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(trips_by_date.index, trips_by_date.values, linewidth=2)\n",
    "plt.title('Number of Taxi Trips Over Time', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average trips per day: {trips_by_date.mean():.0f}\")\n",
    "print(f\"Peak day: {trips_by_date.idxmax()} with {trips_by_date.max()} trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c7c29",
   "metadata": {},
   "source": [
    "### Step 3: Relationships Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fc229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Distance vs Fare\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['trip_distance'], df['fare_amount'], alpha=0.3, s=10)\n",
    "plt.xlabel('Trip Distance (miles)')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.title('Trip Distance vs Fare Amount', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "numeric_cols = ['trip_distance', 'fare_amount', 'tip_amount', 'total_amount', 'passenger_count', 'trip_duration']\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdfde4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Data Cleaning & Preprocessing\n",
    "\n",
    "### Learning Objectives\n",
    "- Identify and handle missing data\n",
    "- Detect and handle outliers\n",
    "- Validate data ranges\n",
    "- Clean data systematically\n",
    "\n",
    "### Step 1: Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MISSING DATA ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Detailed missing data analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': df.isnull().sum().values,\n",
    "    'Missing Percentage': (df.isnull().sum() / len(df) * 100).values,\n",
    "    'Data Type': df.dtypes.values\n",
    "})\n",
    "missing_analysis = missing_analysis[missing_analysis['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "display(missing_analysis)\n",
    "\n",
    "# Visualize missing data pattern\n",
    "if len(missing_analysis) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(missing_analysis['Column'], missing_analysis['Missing Percentage'])\n",
    "    plt.xlabel('Missing Percentage (%)')\n",
    "    plt.title('Missing Data by Column', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8460da",
   "metadata": {},
   "source": [
    "### Step 2: Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea353c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy for handling missing data\n",
    "# For tip_amount: Missing likely means no tip (0), but we'll be conservative\n",
    "# and use median imputation for now\n",
    "\n",
    "print(\"Handling missing data...\")\n",
    "print(f\"Missing tip_amount before: {df['tip_amount'].isnull().sum()}\")\n",
    "\n",
    "# Option 1: Fill with 0 (assuming missing = no tip)\n",
    "# df['tip_amount'] = df['tip_amount'].fillna(0)\n",
    "\n",
    "# Option 2: Fill with median (more conservative)\n",
    "df['tip_amount'] = df['tip_amount'].fillna(df['tip_amount'].median())\n",
    "\n",
    "# Recalculate total_amount\n",
    "df['total_amount'] = df['fare_amount'] + df['tip_amount'] + 0.5\n",
    "\n",
    "print(f\"Missing tip_amount after: {df['tip_amount'].isnull().sum()}\")\n",
    "print(\"Missing data handling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba06528",
   "metadata": {},
   "source": [
    "### Step 3: Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check trip_distance outliers\n",
    "distance_outliers, dist_lower, dist_upper = detect_outliers_iqr(df, 'trip_distance')\n",
    "print(f\"\\nTrip Distance Outliers:\")\n",
    "print(f\"  Lower bound: {dist_lower:.2f} miles\")\n",
    "print(f\"  Upper bound: {dist_upper:.2f} miles\")\n",
    "print(f\"  Number of outliers: {len(distance_outliers):,} ({len(distance_outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Check fare_amount outliers\n",
    "fare_outliers, fare_lower, fare_upper = detect_outliers_iqr(df, 'fare_amount')\n",
    "print(f\"\\nFare Amount Outliers:\")\n",
    "print(f\"  Lower bound: ${fare_lower:.2f}\")\n",
    "print(f\"  Upper bound: ${fare_upper:.2f}\")\n",
    "print(f\"  Number of outliers: {len(fare_outliers):,} ({len(fare_outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Check trip_duration outliers (unrealistic trips)\n",
    "duration_outliers, dur_lower, dur_upper = detect_outliers_iqr(df, 'trip_duration')\n",
    "print(f\"\\nTrip Duration Outliers:\")\n",
    "print(f\"  Lower bound: {dur_lower:.2f} minutes\")\n",
    "print(f\"  Upper bound: {dur_upper:.2f} minutes\")\n",
    "print(f\"  Number of outliers: {len(duration_outliers):,} ({len(duration_outliers)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702aa46",
   "metadata": {},
   "source": [
    "### Step 4: Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1dee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for completely duplicate rows\n",
    "n_duplicates = df.duplicated().sum()\n",
    "print(f\"Completely duplicate rows: {n_duplicates:,}\")\n",
    "\n",
    "# Check for duplicates based on key columns (same trip recorded twice)\n",
    "# NYC TLC data uses location IDs (PULocationID, DOLocationID) instead of lat/long\n",
    "key_cols = ['pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID']\n",
    "# Only check if these columns exist\n",
    "available_key_cols = [col for col in key_cols if col in df.columns]\n",
    "if len(available_key_cols) >= 2:\n",
    "    n_key_duplicates = df.duplicated(subset=available_key_cols).sum()\n",
    "    print(f\"Duplicate trips (same pickup/dropoff location and time): {n_key_duplicates:,}\")\n",
    "else:\n",
    "    print(\"Location columns not available for duplicate detection\")\n",
    "\n",
    "# Show examples if any duplicates exist\n",
    "if n_duplicates > 0:\n",
    "    print(\"\\nExample duplicate rows:\")\n",
    "    display(df[df.duplicated(keep=False)].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b31819",
   "metadata": {},
   "source": [
    "### Step 5: Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers based on domain knowledge\n",
    "print(\"\\nHandling outliers...\")\n",
    "\n",
    "# Remove unrealistic trip distances (> 50 miles in NYC is very unusual)\n",
    "# Or cap them at a reasonable maximum\n",
    "df_clean = df.copy()\n",
    "print(f\"Original shape: {df_clean.shape}\")\n",
    "\n",
    "# Remove duplicate rows (if any)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f\"After removing duplicates: {df_clean.shape}\")\n",
    "\n",
    "# Cap trip_distance at 50 miles (very generous for NYC)\n",
    "df_clean['trip_distance'] = df_clean['trip_distance'].clip(upper=50)\n",
    "\n",
    "# Remove trips with negative or zero distance\n",
    "df_clean = df_clean[df_clean['trip_distance'] > 0]\n",
    "\n",
    "# Remove trips with unrealistic duration (> 2 hours is very unusual)\n",
    "df_clean = df_clean[df_clean['trip_duration'] <= 120]  # 2 hours max\n",
    "\n",
    "# Remove trips with negative fare\n",
    "df_clean = df_clean[df_clean['fare_amount'] > 0]\n",
    "\n",
    "# Remove trips with unrealistic passenger counts\n",
    "df_clean = df_clean[df_clean['passenger_count'].between(1, 6)]\n",
    "\n",
    "print(f\"Cleaned shape: {df_clean.shape}\")\n",
    "print(f\"Removed {len(df) - len(df_clean):,} rows ({(len(df) - len(df_clean))/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f45f5",
   "metadata": {},
   "source": [
    "### Step 6: Data Type Validation and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime columns are properly typed\n",
    "print(\"\\nValidating and converting data types...\")\n",
    "\n",
    "df_clean['pickup_datetime'] = pd.to_datetime(df_clean['pickup_datetime'])\n",
    "df_clean['dropoff_datetime'] = pd.to_datetime(df_clean['dropoff_datetime'])\n",
    "\n",
    "# Ensure numeric columns are proper types\n",
    "numeric_cols = ['trip_distance', 'fare_amount', 'tip_amount', 'total_amount', \n",
    "                'passenger_count', 'trip_duration']\n",
    "for col in numeric_cols:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(\"\\nFinal data quality check:\")\n",
    "print(f\"Missing values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Data types:\\n{df_clean.dtypes}\")\n",
    "print(f\"\\nFinal dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979aec77",
   "metadata": {},
   "source": [
    "### Step 7: Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef821fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset for next notebook\n",
    "output_dir = '../output'\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df_clean.to_csv(f'{output_dir}/01_cleaned_taxi_data.csv', index=False)\n",
    "print(f\"\\nCleaned data saved to: {output_dir}/01_cleaned_taxi_data.csv\")\n",
    "print(\"Ready for next phase: Data Wrangling & Feature Engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5599602",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ✅ **Loaded data** and performed initial inspection\n",
    "2. ✅ **Explored distributions** and relationships\n",
    "3. ✅ **Identified missing data** and handled it appropriately\n",
    "4. ✅ **Detected outliers** using statistical methods\n",
    "5. ✅ **Cleaned data** based on domain knowledge\n",
    "6. ✅ **Validated data types** and ranges\n",
    "7. ✅ **Saved cleaned dataset** for next phase\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Always inspect data before cleaning\n",
    "- Use domain knowledge to guide cleaning decisions\n",
    "- Document your cleaning steps\n",
    "- Save intermediate results\n",
    "\n",
    "**Next:** Notebook 2 will focus on data wrangling, merging, and feature engineering.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
