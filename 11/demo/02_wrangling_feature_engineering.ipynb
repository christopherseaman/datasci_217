{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2710779",
   "metadata": {},
   "source": [
    "# Notebook 2: Wrangling & Feature Engineering\n",
    "\n",
    "**Phases 4-5:** Data Wrangling & Transformation, Feature Engineering & Aggregation\n",
    "\n",
    "**Dataset:** NYC Taxi Trip Dataset (continuing from Notebook 1)\n",
    "\n",
    "**Focus:** Transforming and enriching data - merging datasets, working with datetime data, reshaping, and creating features for modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 4: Data Wrangling & Transformation\n",
    "\n",
    "### Learning Objectives\n",
    "- Merge and join multiple datasets\n",
    "- Handle datetime columns and set datetime index\n",
    "- Extract time-based features\n",
    "- Reshape data for analysis\n",
    "- Work with indexes\n",
    "\n",
    "### Step 1: Load Cleaned Data from Previous Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b069fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Load cleaned data from Notebook 1\n",
    "df = pd.read_csv('../output/01_cleaned_taxi_data.csv')\n",
    "print(f\"Loaded {len(df):,} cleaned taxi trips\")\n",
    "print(f\"Date range: {df['pickup_datetime'].min()} to {df['pickup_datetime'].max()}\")\n",
    "\n",
    "# Check if location IDs are available (they should be if using real NYC TLC data)\n",
    "if 'PULocationID' in df.columns and 'DOLocationID' in df.columns:\n",
    "    print(f\"✓ Location IDs found: {df['PULocationID'].nunique()} unique pickup zones, {df['DOLocationID'].nunique()} unique dropoff zones\")\n",
    "else:\n",
    "    print(\"⚠️  Note: PULocationID/DOLocationID not found - zone lookup will be limited\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ac52b",
   "metadata": {},
   "source": [
    "### Step 2: Convert to Datetime and Set Datetime Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime columns\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "\n",
    "# Recalculate trip_duration if needed\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Set pickup_datetime as index for datetime-based operations\n",
    "df_ts = df.set_index('pickup_datetime').sort_index()\n",
    "\n",
    "print(f\"Datetime index set. Shape: {df_ts.shape}\")\n",
    "print(f\"Index range: {df_ts.index.min()} to {df_ts.index.max()}\")\n",
    "display(df_ts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae8981",
   "metadata": {},
   "source": [
    "### Step 3: Extract Time-Based Features\n",
    "\n",
    "**Why extract time-based features?**\n",
    "- **Temporal patterns:** Hour, day of week, and month reveal important patterns (rush hours, weekends, seasons)\n",
    "- **Modeling:** Time features are often strong predictors (e.g., fare varies by time of day)\n",
    "- **Analysis:** Enable grouping and aggregation by time periods\n",
    "\n",
    "**What time features to extract?**\n",
    "- **Hour (0-23):** Captures daily patterns (morning rush, lunch, evening)\n",
    "- **Day of week (0-6):** Captures weekly patterns (weekdays vs weekends)\n",
    "- **Month (1-12):** Captures seasonal patterns\n",
    "- **Derived features:** Weekend flag, time-of-day categories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract various time-based features from the datetime index\n",
    "df_ts['hour'] = df_ts.index.hour\n",
    "df_ts['day_of_week'] = df_ts.index.dayofweek  # 0=Monday, 6=Sunday\n",
    "df_ts['day_name'] = df_ts.index.day_name()\n",
    "df_ts['month'] = df_ts.index.month\n",
    "df_ts['month_name'] = df_ts.index.month_name()\n",
    "df_ts['year'] = df_ts.index.year\n",
    "df_ts['is_weekend'] = df_ts['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Create time-of-day categories\n",
    "def get_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_ts['time_of_day'] = df_ts['hour'].apply(get_time_of_day)\n",
    "\n",
    "print(\"Time-based features extracted:\")\n",
    "print(df_ts[['hour', 'day_of_week', 'day_name', 'month', 'is_weekend', 'time_of_day']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05242fd0",
   "metadata": {},
   "source": [
    "### Step 4: Merge with Additional Data (Zone Lookup Table)\n",
    "\n",
    "**Note:** NYC TLC data includes `PULocationID` and `DOLocationID` columns. We'll load the official NYC Taxi Zone lookup table (downloaded by `download_data.sh`) and merge it with the trip data to add zone names and boroughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that location IDs are present in the data\n",
    "# Real NYC TLC data includes PULocationID and DOLocationID columns\n",
    "if 'PULocationID' not in df_ts.columns or 'DOLocationID' not in df_ts.columns:\n",
    "    raise ValueError(\"PULocationID and DOLocationID columns not found in data. This is required for zone lookup merge.\")\n",
    "\n",
    "# Load official NYC Taxi Zone Lookup Table\n",
    "# This file is downloaded by download_data.sh and contains all 265 zones with official names\n",
    "zone_lookup_file = 'data/taxi_zone_lookup.csv'\n",
    "\n",
    "if not os.path.exists(zone_lookup_file):\n",
    "    print(\"❌ Zone lookup file not found!\")\n",
    "    print(\"Please run download_data.sh to download the zone lookup file:\")\n",
    "    print(\"  chmod +x download_data.sh\")\n",
    "    print(\"  ./download_data.sh\")\n",
    "    raise FileNotFoundError(f\"Zone lookup file not found: {zone_lookup_file}. Run download_data.sh first.\")\n",
    "\n",
    "# Load official zone lookup file\n",
    "zone_lookup = pd.read_csv(zone_lookup_file)\n",
    "# Rename columns to match our merge needs\n",
    "# Drop 'service_zone' to avoid duplicate columns when merging pickup and dropoff\n",
    "zone_lookup = zone_lookup.rename(columns={'Zone': 'zone_name'}).drop(columns=['service_zone'], errors='ignore')\n",
    "print(f\"✅ Loaded official zone lookup: {len(zone_lookup)} zones\")\n",
    "print(f\"   Columns: {list(zone_lookup.columns)}\")\n",
    "print(f\"   Sample zones: {zone_lookup[['LocationID', 'Borough', 'zone_name']].head(5).to_string(index=False)}\")\n",
    "\n",
    "# Use actual location IDs from the real NYC TLC data\n",
    "# Real data includes PULocationID and DOLocationID columns\n",
    "df_ts_reset = df_ts.reset_index()\n",
    "\n",
    "if 'PULocationID' in df_ts_reset.columns and 'DOLocationID' in df_ts_reset.columns:\n",
    "    # Rename to match zone_lookup column name for merging\n",
    "    df_ts_reset = df_ts_reset.rename(columns={'PULocationID': 'pickup_zone_id', 'DOLocationID': 'dropoff_zone_id'})\n",
    "    print(\"✓ Using real location IDs from NYC TLC data\")\n",
    "    print(f\"  Pickup zones: {df_ts_reset['pickup_zone_id'].nunique()} unique\")\n",
    "    print(f\"  Dropoff zones: {df_ts_reset['dropoff_zone_id'].nunique()} unique\")\n",
    "else:\n",
    "    # This should never execute - we check for PULocationID/DOLocationID above and raise ValueError if missing\n",
    "    # If we somehow get here, we can't do zone assignment without location IDs\n",
    "    raise ValueError(\"PULocationID/DOLocationID columns are required but not found. This should not happen with real NYC TLC data.\")\n",
    "\n",
    "# Merge pickup zone information using LEFT JOIN\n",
    "# LEFT JOIN keeps all rows from left DataFrame (df_ts), adds matching data from right (zone_lookup)\n",
    "# This is the most common join type - we want all trips, even if zone info is missing\n",
    "# IMPORTANT: Reset index before merge, then set it back to preserve DatetimeIndex\n",
    "\n",
    "if 'pickup_zone_id' in df_ts_reset.columns:\n",
    "    # Merge pickup zone information using LEFT JOIN\n",
    "    df_ts_reset = df_ts_reset.merge(\n",
    "        zone_lookup.rename(columns={'LocationID': 'pickup_zone_id', 'zone_name': 'pickup_zone_name', 'Borough': 'pickup_borough'}),\n",
    "        on='pickup_zone_id',\n",
    "        how='left'  # LEFT JOIN: keep all trips, add zone info where available\n",
    "    )\n",
    "    \n",
    "    # Merge dropoff zone information\n",
    "    df_ts_reset = df_ts_reset.merge(\n",
    "        zone_lookup.rename(columns={'LocationID': 'dropoff_zone_id', 'zone_name': 'dropoff_zone_name', 'Borough': 'dropoff_borough'}),\n",
    "        on='dropoff_zone_id',\n",
    "        how='left'  # LEFT JOIN: keep all trips\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Zone information merged:\")\n",
    "    print(f\"   Total columns: {df_ts_reset.shape[1]}\")\n",
    "    print(f\"   Zones matched: {df_ts_reset['pickup_zone_name'].notna().sum():,} / {len(df_ts_reset):,} trips\")\n",
    "    if 'pickup_zone_name' in df_ts_reset.columns:\n",
    "        print(\"\\nSample zone information:\")\n",
    "        display(df_ts_reset[['pickup_zone_name', 'pickup_borough', 'dropoff_zone_name', 'dropoff_borough']].head(10))\n",
    "else:\n",
    "    print(\"⚠️  Zone merge skipped - location IDs not available in data\")\n",
    "\n",
    "# Set datetime index back\n",
    "df_ts = df_ts_reset.set_index('pickup_datetime').sort_index()\n",
    "\n",
    "# Demonstrate other join types (for educational purposes)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"JOIN TYPE EXAMPLES (Educational)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create example DataFrames to demonstrate join types\n",
    "left_df = pd.DataFrame({'key': [1, 2, 3, 4], 'left_value': ['A', 'B', 'C', 'D']})\n",
    "right_df = pd.DataFrame({'key': [2, 3, 4, 5], 'right_value': ['X', 'Y', 'Z', 'W']})\n",
    "\n",
    "print(\"Left DataFrame:\")\n",
    "display(left_df)\n",
    "print(\"\\nRight DataFrame:\")\n",
    "display(right_df)\n",
    "\n",
    "# INNER JOIN: Only rows with matching keys in both DataFrames\n",
    "inner_result = pd.merge(left_df, right_df, on='key', how='inner')\n",
    "print(\"\\nINNER JOIN (only matching keys):\")\n",
    "display(inner_result)\n",
    "\n",
    "# LEFT JOIN: All rows from left, matching from right\n",
    "left_result = pd.merge(left_df, right_df, on='key', how='left')\n",
    "print(\"\\nLEFT JOIN (all from left, matching from right):\")\n",
    "display(left_result)\n",
    "\n",
    "# RIGHT JOIN: All rows from right, matching from left\n",
    "right_result = pd.merge(left_df, right_df, on='key', how='right')\n",
    "print(\"\\nRIGHT JOIN (all from right, matching from left):\")\n",
    "display(right_result)\n",
    "\n",
    "# OUTER JOIN: All rows from both DataFrames\n",
    "outer_result = pd.merge(left_df, right_df, on='key', how='outer')\n",
    "print(\"\\nOUTER JOIN (all rows from both):\")\n",
    "display(outer_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0754a0",
   "metadata": {},
   "source": [
    "### Step 5: Reshape Data - Pivot Table Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf795fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table: Average fare by day of week and time of day\n",
    "pivot_fare = df_ts.pivot_table(\n",
    "    values='fare_amount',\n",
    "    index='day_name',\n",
    "    columns='time_of_day',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(\"Average Fare by Day of Week and Time of Day:\")\n",
    "display(pivot_fare.round(2))\n",
    "\n",
    "# Visualize the pivot table\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_fare, annot=True, fmt='.1f', cmap='YlOrRd', cbar_kws={'label': 'Average Fare ($)'})\n",
    "plt.title('Average Fare Amount by Day and Time of Day', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Day of Week')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514de9c",
   "metadata": {},
   "source": [
    "### Step 6: Reshape Data - Melt Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert wide format to long format\n",
    "# Let's create a summary by hour with multiple metrics\n",
    "\n",
    "hourly_summary = df_ts.groupby('hour').agg({\n",
    "    'fare_amount': 'mean',\n",
    "    'trip_distance': 'mean',\n",
    "    'trip_duration': 'mean',\n",
    "    'passenger_count': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Hourly Summary (Wide Format):\")\n",
    "display(hourly_summary.head())\n",
    "\n",
    "# Melt to long format\n",
    "hourly_long = hourly_summary.melt(\n",
    "    id_vars='hour',\n",
    "    value_vars=['fare_amount', 'trip_distance', 'trip_duration', 'passenger_count'],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "print(\"\\nHourly Summary (Long Format):\")\n",
    "display(hourly_long.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993215e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 5: Feature Engineering & Aggregation\n",
    "\n",
    "### Learning Objectives\n",
    "- Create derived features\n",
    "- Perform groupby aggregations\n",
    "- Calculate rolling window statistics\n",
    "- Create time-based features\n",
    "- Aggregate by multiple dimensions\n",
    "\n",
    "### Step 1: Create Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cd41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed (miles per hour) - derived from distance and duration\n",
    "df_ts['speed_mph'] = df_ts['trip_distance'] / (df_ts['trip_duration'] / 60)  # Convert minutes to hours\n",
    "df_ts['speed_mph'] = df_ts['speed_mph'].replace([np.inf, -np.inf], np.nan)  # Handle division by zero\n",
    "df_ts['speed_mph'] = df_ts['speed_mph'].clip(upper=60)  # Cap at 60 mph (reasonable for NYC)\n",
    "\n",
    "# Fare per mile\n",
    "df_ts['fare_per_mile'] = df_ts['fare_amount'] / df_ts['trip_distance']\n",
    "df_ts['fare_per_mile'] = df_ts['fare_per_mile'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Tip percentage\n",
    "df_ts['tip_percentage'] = (df_ts['tip_amount'] / df_ts['fare_amount']) * 100\n",
    "df_ts['tip_percentage'] = df_ts['tip_percentage'].fillna(0)  # No tip = 0%\n",
    "\n",
    "# Distance category\n",
    "def categorize_distance(dist):\n",
    "    if dist < 1:\n",
    "        return 'Short'\n",
    "    elif dist < 3:\n",
    "        return 'Medium'\n",
    "    elif dist < 10:\n",
    "        return 'Long'\n",
    "    else:\n",
    "        return 'Very Long'\n",
    "\n",
    "df_ts['distance_category'] = df_ts['trip_distance'].apply(categorize_distance)\n",
    "\n",
    "print(\"Derived features created:\")\n",
    "print(df_ts[['speed_mph', 'fare_per_mile', 'tip_percentage', 'distance_category']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0207dd4",
   "metadata": {},
   "source": [
    "### Step 2: GroupBy Aggregations\n",
    "\n",
    "**What is GroupBy?**\n",
    "GroupBy splits data into groups, applies a function to each group, and combines the results. It's one of pandas' most powerful features for data analysis.\n",
    "\n",
    "**Why use GroupBy?**\n",
    "- **Summarize:** Calculate statistics for each group (e.g., average fare by day of week)\n",
    "- **Compare:** See how metrics differ across groups\n",
    "- **Aggregate:** Reduce data size while preserving important patterns\n",
    "- **Explore:** Discover relationships between categorical and numeric variables\n",
    "\n",
    "**Common GroupBy operations:**\n",
    "- **Single column:** `groupby('day_of_week')` - group by one variable\n",
    "- **Multiple columns:** `groupby(['day_of_week', 'time_of_day'])` - group by multiple variables\n",
    "- **Multiple functions:** `agg({'fare': 'mean', 'distance': 'sum'})` - different functions for different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd35b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by day of week\n",
    "daily_stats = df_ts.groupby('day_name').agg({\n",
    "    'fare_amount': ['mean', 'median', 'std', 'count'],\n",
    "    'trip_distance': ['mean', 'median'],\n",
    "    'trip_duration': ['mean', 'median'],\n",
    "    'passenger_count': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Statistics by Day of Week:\")\n",
    "display(daily_stats)\n",
    "\n",
    "# Aggregate by multiple dimensions: day of week and time of day\n",
    "multi_agg = df_ts.groupby(['day_name', 'time_of_day']).agg({\n",
    "    'fare_amount': 'mean',\n",
    "    'trip_distance': 'count'  # Count of trips\n",
    "}).rename(columns={'fare_amount': 'avg_fare', 'trip_distance': 'trip_count'})\n",
    "\n",
    "print(\"\\nAverage Fare by Day and Time:\")\n",
    "display(multi_agg.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ce9ee",
   "metadata": {},
   "source": [
    "### Step 3: Rolling Window Calculations\n",
    "\n",
    "**What are rolling windows?**\n",
    "Rolling windows calculate statistics over a sliding window of time periods. For example, a 7-day rolling mean calculates the average of the current day and the previous 6 days.\n",
    "\n",
    "**Why use rolling windows?**\n",
    "- **Smooth trends:** Remove daily noise to see underlying patterns\n",
    "- **Moving averages:** Common in time series analysis\n",
    "- **Trend detection:** Identify increasing/decreasing trends\n",
    "- **Anomaly detection:** Compare current values to rolling statistics\n",
    "\n",
    "**Common rolling window operations:**\n",
    "- **Rolling mean:** Average over window (smooths data)\n",
    "- **Rolling median:** Median over window (robust to outliers)\n",
    "- **Rolling std:** Standard deviation over window (measures volatility)\n",
    "- **Rolling min/max:** Min/max over window (identifies extremes)\n",
    "\n",
    "**Window size considerations:**\n",
    "- **Small windows (3-7 days):** Capture short-term patterns, more responsive to changes\n",
    "- **Large windows (30+ days):** Capture long-term trends, smoother but less responsive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d070879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to hourly for rolling calculations\n",
    "hourly_trips = df_ts.resample('h').agg({\n",
    "    'fare_amount': ['mean', 'count'],\n",
    "    'trip_distance': 'mean',\n",
    "    'total_amount': 'sum'\n",
    "})\n",
    "hourly_trips.columns = ['fare_amount', 'trip_count', 'trip_distance', 'total_amount']\n",
    "hourly_trips = hourly_trips[['fare_amount', 'trip_distance', 'total_amount', 'trip_count']]\n",
    "\n",
    "# Calculate rolling averages (7-day and 30-day windows)\n",
    "hourly_trips['fare_7d_avg'] = hourly_trips['fare_amount'].rolling(window=7*24, min_periods=1).mean()  # 7 days * 24 hours\n",
    "hourly_trips['fare_30d_avg'] = hourly_trips['fare_amount'].rolling(window=30*24, min_periods=1).mean()  # 30 days * 24 hours\n",
    "\n",
    "# Calculate exponentially weighted moving average\n",
    "hourly_trips['fare_ewm'] = hourly_trips['fare_amount'].ewm(span=7*24, adjust=False).mean()\n",
    "\n",
    "print(\"Rolling window calculations:\")\n",
    "display(hourly_trips[['fare_amount', 'fare_7d_avg', 'fare_30d_avg', 'fare_ewm']].head(20))\n",
    "\n",
    "# Visualize rolling averages\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(hourly_trips.index, hourly_trips['fare_amount'], alpha=0.3, label='Hourly Average', linewidth=1)\n",
    "plt.plot(hourly_trips.index, hourly_trips['fare_7d_avg'], label='7-Day Rolling Average', linewidth=2)\n",
    "plt.plot(hourly_trips.index, hourly_trips['fare_30d_avg'], label='30-Day Rolling Average', linewidth=2)\n",
    "plt.plot(hourly_trips.index, hourly_trips['fare_ewm'], label='Exponentially Weighted', linewidth=2, linestyle='--')\n",
    "plt.title('Fare Amount Trends with Rolling Averages', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Fare Amount ($)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5f596",
   "metadata": {},
   "source": [
    "### Step 4: Time-Based Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by hour of day (across all days)\n",
    "hourly_pattern = df_ts.groupby('hour').agg({\n",
    "    'fare_amount': ['mean', 'count'],\n",
    "    'trip_distance': 'mean',\n",
    "    'total_amount': 'sum'\n",
    "})\n",
    "hourly_pattern.columns = ['fare_amount', 'trip_count', 'trip_distance', 'total_amount']\n",
    "hourly_pattern = hourly_pattern[['fare_amount', 'trip_count', 'trip_distance', 'total_amount']]\n",
    "\n",
    "print(\"Hourly Patterns (aggregated across all days):\")\n",
    "display(hourly_pattern.head(10))\n",
    "\n",
    "# Visualize hourly patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Hourly Patterns in Taxi Trips', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Average fare by hour\n",
    "axes[0, 0].plot(hourly_pattern.index, hourly_pattern['fare_amount'], marker='o', linewidth=2)\n",
    "axes[0, 0].set_title('Average Fare by Hour of Day')\n",
    "axes[0, 0].set_xlabel('Hour')\n",
    "axes[0, 0].set_ylabel('Average Fare ($)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trip count by hour\n",
    "axes[0, 1].bar(hourly_pattern.index, hourly_pattern['trip_count'], alpha=0.7)\n",
    "axes[0, 1].set_title('Number of Trips by Hour of Day')\n",
    "axes[0, 1].set_xlabel('Hour')\n",
    "axes[0, 1].set_ylabel('Number of Trips')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Average distance by hour\n",
    "axes[1, 0].plot(hourly_pattern.index, hourly_pattern['trip_distance'], marker='s', color='green', linewidth=2)\n",
    "axes[1, 0].set_title('Average Distance by Hour of Day')\n",
    "axes[1, 0].set_xlabel('Hour')\n",
    "axes[1, 0].set_ylabel('Average Distance (miles)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total revenue by hour\n",
    "axes[1, 1].bar(hourly_pattern.index, hourly_pattern['total_amount'], alpha=0.7, color='orange')\n",
    "axes[1, 1].set_title('Total Revenue by Hour of Day')\n",
    "axes[1, 1].set_xlabel('Hour')\n",
    "axes[1, 1].set_ylabel('Total Revenue ($)')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35624b2",
   "metadata": {},
   "source": [
    "### Step 5: Cross-Tabulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663515fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation: Day of week vs Time of day\n",
    "crosstab = pd.crosstab(\n",
    "    df_ts['day_name'],\n",
    "    df_ts['time_of_day'],\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "print(\"Trip Count: Day of Week × Time of Day\")\n",
    "display(crosstab)\n",
    "\n",
    "# Cross-tabulation with aggregation\n",
    "crosstab_fare = pd.crosstab(\n",
    "    df_ts['day_name'],\n",
    "    df_ts['time_of_day'],\n",
    "    values=df_ts['fare_amount'],\n",
    "    aggfunc='mean',\n",
    "    margins=True\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nAverage Fare: Day of Week × Time of Day\")\n",
    "display(crosstab_fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6383008",
   "metadata": {},
   "source": [
    "### Step 6: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfb4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to make pickup_datetime a regular column again\n",
    "df_processed = df_ts.reset_index()\n",
    "\n",
    "# Save processed dataset for next notebook\n",
    "df_processed.to_csv('../output/02_processed_taxi_data.csv', index=False)\n",
    "print(f\"\\nProcessed data saved: {len(df_processed):,} trips\")\n",
    "print(f\"Columns: {df_processed.shape[1]}\")\n",
    "print(\"\\nReady for next phase: Pattern Analysis & Modeling Prep!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc34d7c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ✅ **Set datetime index** for time-based operations\n",
    "2. ✅ **Extracted time-based features** (hour, day, month, etc.)\n",
    "3. ✅ **Merged zone lookup data** using pandas merge\n",
    "4. ✅ **Reshaped data** using pivot and melt\n",
    "5. ✅ **Created derived features** (speed, fare per mile, etc.)\n",
    "6. ✅ **Performed aggregations** by multiple dimensions\n",
    "7. ✅ **Calculated rolling windows** for trend analysis\n",
    "8. ✅ **Created time-based patterns** and visualizations\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Datetime indexing enables time-based operations and aggregations\n",
    "- Merging enriches data with additional context\n",
    "- Feature engineering creates predictive signals\n",
    "- Rolling windows reveal trends and patterns\n",
    "- GroupBy aggregations summarize data at different levels\n",
    "\n",
    "**Next:** Notebook 3 will focus on pattern analysis, advanced visualizations, and preparing data for modeling.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
