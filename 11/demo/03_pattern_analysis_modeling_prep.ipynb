{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d89544",
   "metadata": {},
   "source": [
    "# Notebook 3: Pattern Analysis & Modeling Prep\n",
    "\n",
    "**Phases 6-7:** Pattern Analysis & Advanced Visualization, Modeling Preparation\n",
    "\n",
    "**Dataset:** NYC Taxi Trip Dataset (continuing from Notebook 2)\n",
    "\n",
    "**Focus:** Deep analysis of patterns, advanced visualizations, and preparing data for predictive modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 6: Pattern Analysis & Advanced Visualization\n",
    "\n",
    "### Learning Objectives\n",
    "- Create advanced multi-panel visualizations\n",
    "- Identify trends and seasonal patterns\n",
    "- Perform statistical analysis\n",
    "- Visualize relationships across multiple dimensions\n",
    "\n",
    "### Step 1: Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ce33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Load processed data from Notebook 2\n",
    "df = pd.read_csv('../output/02_processed_taxi_data.csv')\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "print(f\"Loaded {len(df):,} processed trips\")\n",
    "print(f\"Date range: {df['pickup_datetime'].min()} to {df['pickup_datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa29f96",
   "metadata": {},
   "source": [
    "### Step 2: Trends Analysis Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set datetime index for time-based operations\n",
    "df_ts = df.set_index('pickup_datetime').sort_index()\n",
    "\n",
    "# Resample to daily for trend analysis\n",
    "daily = df_ts.resample('d').agg({\n",
    "    'fare_amount': 'mean',\n",
    "    'total_amount': 'sum',\n",
    "    'trip_distance': 'count'  # Count trips\n",
    "}).rename(columns={'trip_distance': 'trip_count'})\n",
    "\n",
    "# Calculate moving averages for trend detection\n",
    "daily['fare_7d_ma'] = daily['fare_amount'].rolling(window=7, min_periods=1).mean()\n",
    "daily['fare_30d_ma'] = daily['fare_amount'].rolling(window=30, min_periods=1).mean()\n",
    "\n",
    "# Visualize trends\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "fig.suptitle('Trends Analysis Over Time', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Average fare over time\n",
    "axes[0].plot(daily.index, daily['fare_amount'], alpha=0.5, label='Daily Average', linewidth=1)\n",
    "axes[0].plot(daily.index, daily['fare_7d_ma'], label='7-Day Moving Average', linewidth=2)\n",
    "axes[0].plot(daily.index, daily['fare_30d_ma'], label='30-Day Moving Average', linewidth=2)\n",
    "axes[0].set_title('Average Fare Amount Over Time')\n",
    "axes[0].set_ylabel('Fare Amount ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total revenue over time\n",
    "axes[1].plot(daily.index, daily['total_amount'], linewidth=2, color='green')\n",
    "axes[1].set_title('Total Daily Revenue')\n",
    "axes[1].set_ylabel('Revenue ($)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Trip count over time\n",
    "axes[2].bar(daily.index, daily['trip_count'], alpha=0.7, width=1)\n",
    "axes[2].set_title('Number of Trips per Day')\n",
    "axes[2].set_ylabel('Trip Count')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35ffaa",
   "metadata": {},
   "source": [
    "### Step 3: Seasonal Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze patterns by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df_ts['day_name'] = df_ts.index.day_name()\n",
    "\n",
    "daily_by_dow = df_ts.groupby('day_name')['fare_amount'].agg(['mean', 'std', 'count']).reindex(day_order)\n",
    "\n",
    "# Analyze patterns by month\n",
    "monthly = df_ts.groupby('month')['fare_amount'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "# Visualize seasonal patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Seasonal Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Average fare by day of week\n",
    "axes[0, 0].bar(range(len(daily_by_dow)), daily_by_dow['mean'], alpha=0.7)\n",
    "axes[0, 0].set_xticks(range(len(daily_by_dow)))\n",
    "axes[0, 0].set_xticklabels(daily_by_dow.index, rotation=45, ha='right')\n",
    "axes[0, 0].set_title('Average Fare by Day of Week')\n",
    "axes[0, 0].set_ylabel('Average Fare ($)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Average fare by month\n",
    "axes[0, 1].plot(monthly.index, monthly['mean'], marker='o', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_title('Average Fare by Month')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Average Fare ($)')\n",
    "axes[0, 1].set_xticks(monthly.index)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Hourly pattern (heatmap by day of week)\n",
    "hourly_dow = df_ts.groupby(['day_name', 'hour'])['fare_amount'].mean().unstack(level=0).reindex(columns=day_order)\n",
    "sns.heatmap(hourly_dow, annot=False, cmap='YlOrRd', ax=axes[1, 0], cbar_kws={'label': 'Avg Fare ($)'})\n",
    "axes[1, 0].set_title('Average Fare: Hour × Day of Week')\n",
    "axes[1, 0].set_xlabel('Day of Week')\n",
    "axes[1, 0].set_ylabel('Hour of Day')\n",
    "\n",
    "# Weekend vs weekday comparison\n",
    "df_ts['is_weekend'] = df_ts['day_of_week'].isin([5, 6])\n",
    "weekend_comparison = df_ts.groupby(['is_weekend', 'hour'])['fare_amount'].mean().unstack(level=0)\n",
    "axes[1, 1].plot(weekend_comparison.index, weekend_comparison[False], label='Weekday', marker='o', linewidth=2)\n",
    "axes[1, 1].plot(weekend_comparison.index, weekend_comparison[True], label='Weekend', marker='s', linewidth=2)\n",
    "axes[1, 1].set_title('Average Fare: Weekday vs Weekend by Hour')\n",
    "axes[1, 1].set_xlabel('Hour of Day')\n",
    "axes[1, 1].set_ylabel('Average Fare ($)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd18c64",
   "metadata": {},
   "source": [
    "### Step 4: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01447bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features for correlation\n",
    "numeric_features = ['fare_amount', 'trip_distance', 'trip_duration', 'passenger_count', \n",
    "                    'tip_amount', 'total_amount', 'speed_mph', 'fare_per_mile', 'tip_percentage']\n",
    "\n",
    "corr_matrix = df_ts[numeric_features].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix: Key Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify strongest correlations\n",
    "print(\"Strongest Correlations (absolute value > 0.5):\")\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "\n",
    "for feat1, feat2, corr in sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"  {feat1} ↔ {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820300b",
   "metadata": {},
   "source": [
    "### Step 5: Multi-Dimensional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dba8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fare by multiple dimensions: distance category, time of day, day type\n",
    "multi_dim = df_ts.groupby(['distance_category', 'time_of_day', 'is_weekend'])['fare_amount'].mean().unstack(level=2)\n",
    "\n",
    "# Note: unstack converts boolean to int (False=0, True=1)\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Multi-Dimensional Fare Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distance category vs time of day (weekday, is_weekend=0)\n",
    "# Need to unstack the Series to create a 2D DataFrame for heatmap\n",
    "weekday_col = 0 if 0 in multi_dim.columns else multi_dim.columns[0]\n",
    "weekday_data = multi_dim[weekday_col].unstack(level=1)  # Unstack time_of_day to columns\n",
    "sns.heatmap(weekday_data, annot=True, fmt='.1f', cmap='YlOrRd', ax=axes[0, 0], cbar_kws={'label': 'Avg Fare ($)'})\n",
    "axes[0, 0].set_title('Weekday: Distance Category × Time of Day')\n",
    "axes[0, 0].set_xlabel('Time of Day')\n",
    "axes[0, 0].set_ylabel('Distance Category')\n",
    "\n",
    "# Distance category vs time of day (weekend, is_weekend=1)\n",
    "weekend_col = 1 if 1 in multi_dim.columns else multi_dim.columns[-1]\n",
    "weekend_data = multi_dim[weekend_col].unstack(level=1)  # Unstack time_of_day to columns\n",
    "sns.heatmap(weekend_data, annot=True, fmt='.1f', cmap='YlOrRd', ax=axes[0, 1], cbar_kws={'label': 'Avg Fare ($)'})\n",
    "axes[0, 1].set_title('Weekend: Distance Category × Time of Day')\n",
    "axes[0, 1].set_xlabel('Time of Day')\n",
    "axes[0, 1].set_ylabel('Distance Category')\n",
    "\n",
    "# Box plot: Fare by distance category\n",
    "sns.boxplot(data=df_ts, x='distance_category', y='fare_amount', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Fare Distribution by Distance Category')\n",
    "axes[1, 0].set_xlabel('Distance Category')\n",
    "axes[1, 0].set_ylabel('Fare Amount ($)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Violin plot: Fare by time of day\n",
    "sns.violinplot(data=df_ts, x='time_of_day', y='fare_amount', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Fare Distribution by Time of Day')\n",
    "axes[1, 1].set_xlabel('Time of Day')\n",
    "axes[1, 1].set_ylabel('Fare Amount ($)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e6ec6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 7: Modeling Preparation\n",
    "\n",
    "### Learning Objectives\n",
    "- Split data temporally for time series\n",
    "- Select and prepare features\n",
    "- Handle categorical variables\n",
    "- Create final modeling dataset\n",
    "\n",
    "### Step 1: Temporal Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data with temporal structure, we must split by time (not randomly)\n",
    "# Train on earlier data, test on later data\n",
    "\n",
    "# Sort by datetime to ensure temporal order\n",
    "df_model = df_ts.reset_index().sort_values('pickup_datetime').copy()\n",
    "\n",
    "# Define split point (e.g., 80% train, 20% test)\n",
    "split_date = df_model['pickup_datetime'].quantile(0.8)\n",
    "print(f\"Split date: {split_date}\")\n",
    "print(f\"Train: {df_model[df_model['pickup_datetime'] < split_date].shape[0]:,} trips\")\n",
    "print(f\"Test: {df_model[df_model['pickup_datetime'] >= split_date].shape[0]:,} trips\")\n",
    "\n",
    "# Create train/test split\n",
    "train = df_model[df_model['pickup_datetime'] < split_date].copy()\n",
    "test = df_model[df_model['pickup_datetime'] >= split_date].copy()\n",
    "\n",
    "print(f\"\\nTrain date range: {train['pickup_datetime'].min()} to {train['pickup_datetime'].max()}\")\n",
    "print(f\"Test date range: {test['pickup_datetime'].min()} to {test['pickup_datetime'].max()}\")\n",
    "\n",
    "# Visualize the split\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(train['pickup_datetime'], train['fare_amount'], alpha=0.3, label='Train', linewidth=0.5)\n",
    "plt.plot(test['pickup_datetime'], test['fare_amount'], alpha=0.3, label='Test', linewidth=0.5, color='red')\n",
    "plt.axvline(split_date, color='black', linestyle='--', linewidth=2, label='Split Point')\n",
    "plt.title('Temporal Train/Test Split', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01532a3d",
   "metadata": {},
   "source": [
    "### Step 2: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "target = 'fare_amount'\n",
    "\n",
    "# Select features for modeling\n",
    "# Include temporal, geographic, and trip characteristics\n",
    "feature_cols = [\n",
    "    # Temporal features\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
    "    # Trip characteristics\n",
    "    'trip_distance', 'passenger_count', 'trip_duration',\n",
    "    # Derived features\n",
    "    'speed_mph', 'fare_per_mile',\n",
    "    # Categorical (will need encoding)\n",
    "    'time_of_day', 'distance_category', 'pickup_borough'\n",
    "]\n",
    "\n",
    "# Check feature availability\n",
    "available_features = [f for f in feature_cols if f in df_model.columns]\n",
    "missing_features = [f for f in feature_cols if f not in df_model.columns]\n",
    "\n",
    "print(\"Available features:\", available_features)\n",
    "if missing_features:\n",
    "    print(\"Missing features (will create or skip):\", missing_features)\n",
    "\n",
    "# Select available features\n",
    "X_train = train[available_features].copy()\n",
    "X_test = test[available_features].copy()\n",
    "y_train = train[target].copy()\n",
    "y_test = test[target].copy()\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a8fb8",
   "metadata": {},
   "source": [
    "### Step 3: Handle Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392abe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Categorical features:\", categorical_cols)\n",
    "print(\"Numeric features:\", numeric_cols)\n",
    "\n",
    "# For simplicity, we'll use pandas get_dummies for one-hot encoding\n",
    "# In practice, you might use sklearn's OneHotEncoder\n",
    "\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure test set has same columns as training set\n",
    "# Add missing columns (with 0s) and remove extra columns\n",
    "for col in X_train_encoded.columns:\n",
    "    if col not in X_test_encoded.columns:\n",
    "        X_test_encoded[col] = 0\n",
    "\n",
    "X_test_encoded = X_test_encoded[X_train_encoded.columns]\n",
    "\n",
    "print(f\"\\nAfter encoding:\")\n",
    "print(f\"Training features: {X_train_encoded.shape}\")\n",
    "print(f\"Test features: {X_test_encoded.shape}\")\n",
    "print(f\"Feature names: {list(X_train_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69fcc6",
   "metadata": {},
   "source": [
    "### Step 4: Handle Missing Values in Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training set:\")\n",
    "print(X_train_encoded.isnull().sum()[X_train_encoded.isnull().sum() > 0])\n",
    "\n",
    "# Fill missing values (using training set statistics)\n",
    "# For numeric columns, use median\n",
    "for col in numeric_cols:\n",
    "    if col in X_train_encoded.columns:\n",
    "        median_val = X_train_encoded[col].median()\n",
    "        X_train_encoded[col] = X_train_encoded[col].fillna(median_val)\n",
    "        X_test_encoded[col] = X_test_encoded[col].fillna(median_val)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(f\"Train: {X_train_encoded.isnull().sum().sum()}\")\n",
    "print(f\"Test: {X_test_encoded.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f63149",
   "metadata": {},
   "source": [
    "### Step 5: Save Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ec3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prepared datasets for modeling\n",
    "X_train_encoded.to_csv('../output/03_X_train.csv', index=False)\n",
    "X_test_encoded.to_csv('../output/03_X_test.csv', index=False)\n",
    "y_train.to_csv('../output/03_y_train.csv', index=False)\n",
    "y_test.to_csv('../output/03_y_test.csv', index=False)\n",
    "\n",
    "print(\"Prepared datasets saved:\")\n",
    "print(f\"  X_train: {X_train_encoded.shape}\")\n",
    "print(f\"  X_test: {X_test_encoded.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "print(\"\\nReady for next phase: Modeling & Results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347b976",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ✅ **Analyzed trends over time** with moving averages\n",
    "2. ✅ **Identified seasonal patterns** (day of week, month, hour)\n",
    "3. ✅ **Performed correlation analysis** to understand relationships\n",
    "4. ✅ **Created advanced visualizations** (heatmaps, multi-panel plots)\n",
    "5. ✅ **Split data temporally** to prevent data leakage\n",
    "6. ✅ **Selected and prepared features** for modeling\n",
    "7. ✅ **Handled categorical variables** with encoding\n",
    "8. ✅ **Prepared final datasets** for modeling\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Temporal splits prevent data leakage when data has time structure\n",
    "- Feature engineering creates predictive signals\n",
    "- Categorical encoding is essential for ML models\n",
    "- Advanced visualizations reveal hidden patterns\n",
    "- Proper preparation ensures model quality\n",
    "\n",
    "**Next:** Notebook 4 will build and evaluate predictive models.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
